{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "As my image captioning system is constucted of CNN and RNN, CNN is made of ResNet101 mainly with predetemined weights.\n",
    "\n",
    "According to [this paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf ), ResNet101 has good accuracy compared to ResNet50, but the accuracy of ResNet152 seems to be saturated. That's why I chose ResNet101.\n",
    "Output from ResNet101 is consumed by linear layer, and linear layer output the feature data.\n",
    "\n",
    "RNN is made of embedding, LSTM, and linear layer. Embedding layer absorbs captions to generate embedding data. LSTM consumes features extracted by CNN and embedding data. As manner of usage LSTM, I catenate the feature and the embedding data.\n",
    "\n",
    "To decide each parameters, I referred to [this brilliant paper](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf) because proposition in this paper is very simple and more powerful among various models proposed over the world.\n",
    "I set `hidden_size` as 512, `vocab_threshold` as 5 because this google paper explained so.\n",
    "I chose 512 for `embed_size` because I want `hidden_size` to be equal to `embed_size`.\n",
    "\n",
    "More layer gave better accuracy in [this paper](https://arxiv.org/pdf/1805.09137) also.\n",
    "I chose 16 for `batch size`.\n",
    "\n",
    "\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.68s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/414113 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 667/414113 [00:00<01:02, 6609.54it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1478/414113 [00:00<00:56, 7358.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 2279/414113 [00:00<00:54, 7573.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 3056/414113 [00:00<00:53, 7622.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 3840/414113 [00:00<00:53, 7664.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 4648/414113 [00:00<00:52, 7733.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 5483/414113 [00:00<00:52, 7818.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 6281/414113 [00:00<00:52, 7838.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 7079/414113 [00:00<00:51, 7853.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 7877/414113 [00:01<00:51, 7866.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 8667/414113 [00:01<00:51, 7868.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 9457/414113 [00:01<00:51, 7871.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 10274/414113 [00:01<00:51, 7892.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 11067/414113 [00:01<00:51, 7888.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 11857/414113 [00:01<00:51, 7880.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 12645/414113 [00:01<00:51, 7857.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 13422/414113 [00:01<00:51, 7846.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 14196/414113 [00:01<00:51, 7834.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▎         | 14973/414113 [00:01<00:50, 7829.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 15764/414113 [00:02<00:50, 7833.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 16565/414113 [00:02<00:50, 7841.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 17350/414113 [00:02<00:50, 7835.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 18165/414113 [00:02<00:50, 7849.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 18956/414113 [00:02<00:50, 7849.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 19766/414113 [00:02<00:50, 7858.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 20586/414113 [00:02<00:49, 7871.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 21410/414113 [00:02<00:49, 7885.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 22283/414113 [00:02<00:49, 7915.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 23143/414113 [00:02<00:49, 7938.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 23981/414113 [00:03<00:49, 7948.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 24815/414113 [00:03<00:48, 7953.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 25642/414113 [00:03<00:48, 7959.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 26465/414113 [00:03<00:48, 7966.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 27287/414113 [00:03<00:48, 7965.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 28157/414113 [00:03<00:48, 7985.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 28987/414113 [00:03<00:48, 7992.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 29815/414113 [00:03<00:48, 7993.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 30635/414113 [00:03<00:47, 7998.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 31505/414113 [00:03<00:47, 8015.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 32349/414113 [00:04<00:47, 8026.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 33186/414113 [00:04<00:47, 8025.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 34014/414113 [00:04<00:47, 8020.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 34827/414113 [00:04<00:49, 7617.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 35634/414113 [00:04<00:49, 7627.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 36433/414113 [00:04<00:49, 7635.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 37246/414113 [00:04<00:49, 7645.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 38017/414113 [00:04<00:49, 7645.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 38821/414113 [00:05<00:49, 7653.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 39592/414113 [00:05<00:48, 7652.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 40360/414113 [00:05<00:48, 7648.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 41177/414113 [00:05<00:48, 7658.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 42001/414113 [00:05<00:48, 7668.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 42797/414113 [00:05<00:48, 7673.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 43590/414113 [00:05<00:48, 7677.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 44382/414113 [00:05<00:48, 7667.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 45166/414113 [00:05<00:48, 7670.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 45974/414113 [00:05<00:47, 7676.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 46774/414113 [00:06<00:47, 7681.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 47569/414113 [00:06<00:47, 7686.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 48373/414113 [00:06<00:47, 7691.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 49192/414113 [00:06<00:47, 7699.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 50030/414113 [00:06<00:47, 7709.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 50855/414113 [00:06<00:47, 7718.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 51687/414113 [00:06<00:46, 7727.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 52508/414113 [00:06<00:46, 7732.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 53325/414113 [00:06<00:46, 7735.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 54154/414113 [00:06<00:46, 7743.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 54979/414113 [00:07<00:46, 7750.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 55866/414113 [00:07<00:46, 7766.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▎        | 56760/414113 [00:07<00:45, 7782.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 57648/414113 [00:07<00:45, 7797.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 58537/414113 [00:07<00:45, 7811.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 59409/414113 [00:07<00:45, 7817.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 60289/414113 [00:07<00:45, 7830.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 61154/414113 [00:07<00:45, 7838.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 62037/414113 [00:07<00:44, 7850.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 62903/414113 [00:08<00:44, 7856.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 63759/414113 [00:08<00:44, 7861.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 64605/414113 [00:08<00:44, 7867.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 65449/414113 [00:08<00:44, 7873.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 66290/414113 [00:08<00:44, 7879.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 67144/414113 [00:08<00:43, 7886.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 67988/414113 [00:08<00:43, 7891.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 68855/414113 [00:08<00:43, 7900.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 69703/414113 [00:08<00:43, 7903.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 70542/414113 [00:08<00:43, 7909.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 71381/414113 [00:09<00:43, 7912.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 72216/414113 [00:09<00:43, 7914.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 73042/414113 [00:09<00:43, 7917.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 73865/414113 [00:09<00:42, 7919.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 74686/414113 [00:09<00:42, 7922.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 75506/414113 [00:09<00:42, 7924.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 76325/414113 [00:09<00:42, 7927.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▊        | 77155/414113 [00:09<00:42, 7931.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 77977/414113 [00:09<00:42, 7933.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 78813/414113 [00:09<00:42, 7937.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 79656/414113 [00:10<00:42, 7942.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 80487/414113 [00:10<00:41, 7945.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 81314/414113 [00:10<00:41, 7945.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 82132/414113 [00:10<00:41, 7947.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 82960/414113 [00:10<00:41, 7950.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 83810/414113 [00:10<00:41, 7955.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 84641/414113 [00:10<00:41, 7958.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 85471/414113 [00:10<00:41, 7958.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 86303/414113 [00:10<00:41, 7962.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 87154/414113 [00:10<00:41, 7967.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 87986/414113 [00:11<00:40, 7968.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██▏       | 88814/414113 [00:11<00:40, 7971.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 89645/414113 [00:11<00:40, 7974.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 90507/414113 [00:11<00:40, 7979.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 91345/414113 [00:11<00:40, 7982.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 92182/414113 [00:11<00:40, 7986.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 93018/414113 [00:11<00:40, 7988.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 93878/414113 [00:11<00:40, 7993.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 94749/414113 [00:11<00:39, 8000.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 95624/414113 [00:11<00:39, 8006.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 96482/414113 [00:12<00:39, 8009.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▎       | 97349/414113 [00:12<00:39, 8015.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▎       | 98207/414113 [00:12<00:39, 8018.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 99085/414113 [00:12<00:39, 8024.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 99945/414113 [00:12<00:39, 8027.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 100800/414113 [00:12<00:39, 8027.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 101639/414113 [00:12<00:38, 8028.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 102470/414113 [00:12<00:38, 8021.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 103270/414113 [00:12<00:38, 8021.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 104070/414113 [00:12<00:38, 8020.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 104898/414113 [00:13<00:38, 8022.76it/s]\u001b[A\u001b[A\n",
      " 35%|███▌      | 146152/414113 [00:33<01:02, 4307.96it/s]\u001b[A\n",
      "\n",
      " 26%|██▌       | 105706/414113 [00:13<00:38, 8021.77it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 106512/414113 [00:13<00:38, 8022.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 107329/414113 [00:13<00:38, 8023.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 108137/414113 [00:13<00:38, 8022.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▋       | 108981/414113 [00:13<00:38, 8025.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 109830/414113 [00:13<00:37, 8029.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 110657/414113 [00:13<00:37, 8030.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 111483/414113 [00:13<00:37, 8024.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 112280/414113 [00:13<00:37, 8024.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 113157/414113 [00:14<00:37, 8029.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 114054/414113 [00:14<00:37, 8036.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 114921/414113 [00:14<00:37, 8040.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 115788/414113 [00:14<00:37, 8044.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 116653/414113 [00:14<00:36, 8048.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 117511/414113 [00:14<00:36, 8051.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▊       | 118375/414113 [00:14<00:36, 8055.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 119253/414113 [00:14<00:36, 8060.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 120117/414113 [00:14<00:36, 8064.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 121001/414113 [00:14<00:36, 8069.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 121889/414113 [00:15<00:36, 8074.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 122776/414113 [00:15<00:36, 8079.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 123654/414113 [00:15<00:35, 8084.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 124546/414113 [00:15<00:35, 8089.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 125427/414113 [00:15<00:35, 8094.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 126308/414113 [00:15<00:35, 8097.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 127185/414113 [00:15<00:35, 8101.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 128060/414113 [00:15<00:35, 8104.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 128932/414113 [00:15<00:35, 8107.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███▏      | 129812/414113 [00:16<00:35, 8111.69it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 130698/414113 [00:16<00:34, 8116.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 131578/414113 [00:16<00:34, 8120.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 132458/414113 [00:16<00:34, 8124.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 133348/414113 [00:16<00:34, 8129.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 134229/414113 [00:16<00:34, 8131.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 135108/414113 [00:16<00:34, 8135.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 135989/414113 [00:16<00:34, 8139.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 136864/414113 [00:16<00:34, 8142.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 137737/414113 [00:16<00:33, 8145.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 138616/414113 [00:17<00:33, 8149.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▎      | 139495/414113 [00:17<00:33, 8153.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 140372/414113 [00:17<00:33, 8156.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 141249/414113 [00:17<00:33, 8160.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 142125/414113 [00:17<00:33, 8162.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 143012/414113 [00:17<00:33, 8166.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 143888/414113 [00:17<00:33, 8165.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 144742/414113 [00:17<00:32, 8166.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 145591/414113 [00:17<00:32, 8166.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 146432/414113 [00:17<00:32, 8164.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 147273/414113 [00:18<00:32, 8165.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 148144/414113 [00:18<00:32, 8168.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 149021/414113 [00:18<00:32, 8171.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 149872/414113 [00:18<00:32, 8171.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▋      | 150712/414113 [00:18<00:32, 8172.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 151549/414113 [00:18<00:32, 8058.69it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 152389/414113 [00:18<00:32, 8060.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 153239/414113 [00:19<00:32, 8062.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 154113/414113 [00:19<00:32, 8066.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 154938/414113 [00:19<00:32, 8067.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 155787/414113 [00:19<00:32, 8069.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 156634/414113 [00:19<00:31, 8071.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 157458/414113 [00:19<00:31, 8069.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 158266/414113 [00:19<00:31, 8069.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 159074/414113 [00:19<00:31, 8069.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▊      | 159921/414113 [00:19<00:31, 8071.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 160740/414113 [00:19<00:31, 8071.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 161559/414113 [00:20<00:31, 8070.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 162369/414113 [00:20<00:31, 8070.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 163185/414113 [00:20<00:31, 8070.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 163994/414113 [00:20<00:30, 8070.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 164799/414113 [00:20<00:30, 8066.69it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 165612/414113 [00:20<00:30, 8067.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 166428/414113 [00:20<00:30, 8067.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 167229/414113 [00:20<00:30, 8066.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 168024/414113 [00:20<00:30, 8065.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 168842/414113 [00:20<00:30, 8065.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 169666/414113 [00:21<00:30, 8066.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 170487/414113 [00:21<00:30, 8067.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████▏     | 171311/414113 [00:21<00:30, 8067.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 172133/414113 [00:21<00:29, 8068.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 172950/414113 [00:21<00:29, 8069.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 173785/414113 [00:21<00:29, 8070.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 174607/414113 [00:21<00:29, 8070.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 175431/414113 [00:21<00:29, 8071.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 176259/414113 [00:21<00:29, 8072.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 177082/414113 [00:21<00:29, 8071.69it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 177904/414113 [00:22<00:29, 8072.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 178737/414113 [00:22<00:29, 8073.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 179565/414113 [00:22<00:29, 8074.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▎     | 180389/414113 [00:22<00:28, 8073.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 181203/414113 [00:22<00:28, 8073.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 182039/414113 [00:22<00:28, 8075.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 182859/414113 [00:22<00:28, 8074.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 183673/414113 [00:22<00:28, 8074.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 184484/414113 [00:22<00:28, 8073.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 185290/414113 [00:22<00:28, 8071.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 186147/414113 [00:23<00:28, 8073.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 186983/414113 [00:23<00:28, 8074.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 187801/414113 [00:23<00:28, 8074.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 188652/414113 [00:23<00:27, 8075.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 189474/414113 [00:23<00:27, 8076.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 190299/414113 [00:23<00:27, 8076.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 191120/414113 [00:23<00:27, 8077.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▋     | 191941/414113 [00:23<00:27, 8077.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 192761/414113 [00:23<00:27, 8077.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 193579/414113 [00:23<00:27, 8078.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 194420/414113 [00:24<00:27, 8079.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 195252/414113 [00:24<00:27, 8080.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 196127/414113 [00:24<00:26, 8083.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 196968/414113 [00:24<00:26, 8083.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 197798/414113 [00:24<00:26, 8084.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 198678/414113 [00:24<00:26, 8086.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 199523/414113 [00:24<00:26, 8088.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 200368/414113 [00:24<00:26, 8089.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▊     | 201212/414113 [00:24<00:26, 8090.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 202051/414113 [00:24<00:26, 8085.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 202849/414113 [00:25<00:26, 8082.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 203688/414113 [00:25<00:26, 8083.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 204487/414113 [00:25<00:25, 8082.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 205327/414113 [00:25<00:25, 8083.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 206183/414113 [00:25<00:25, 8085.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 207003/414113 [00:25<00:25, 8085.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 207828/414113 [00:25<00:25, 8086.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 208658/414113 [00:25<00:25, 8086.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 209484/414113 [00:25<00:25, 8087.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 210316/414113 [00:26<00:25, 8088.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 211143/414113 [00:26<00:25, 8088.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 212040/414113 [00:26<00:24, 8092.22it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 212937/414113 [00:26<00:24, 8095.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 213799/414113 [00:26<00:24, 8097.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 214685/414113 [00:26<00:24, 8100.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 215552/414113 [00:26<00:24, 8097.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 216418/414113 [00:26<00:24, 8099.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 217288/414113 [00:26<00:24, 8101.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 218138/414113 [00:26<00:24, 8101.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 218980/414113 [00:27<00:24, 8102.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 219828/414113 [00:27<00:23, 8104.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 220671/414113 [00:27<00:23, 8103.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 221505/414113 [00:27<00:23, 8104.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▎    | 222351/414113 [00:27<00:23, 8105.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 223188/414113 [00:27<00:23, 8106.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 224034/414113 [00:27<00:23, 8107.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 224873/414113 [00:27<00:23, 8108.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 225711/414113 [00:27<00:23, 8108.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 226566/414113 [00:27<00:23, 8109.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 227401/414113 [00:28<00:23, 8109.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 228249/414113 [00:28<00:22, 8111.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 229097/414113 [00:28<00:22, 8112.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 229936/414113 [00:28<00:22, 8111.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 230778/414113 [00:28<00:22, 8112.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 231604/414113 [00:28<00:22, 8112.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 232448/414113 [00:28<00:22, 8113.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▋    | 233277/414113 [00:28<00:22, 8113.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 234133/414113 [00:28<00:22, 8114.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 234964/414113 [00:28<00:22, 8112.57it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 235774/414113 [00:29<00:21, 8111.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 236574/414113 [00:29<00:21, 8110.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 237385/414113 [00:29<00:21, 8110.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 238244/414113 [00:29<00:21, 8112.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 239080/414113 [00:29<00:21, 8113.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 239919/414113 [00:29<00:21, 8114.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 240748/414113 [00:29<00:21, 8113.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 241580/414113 [00:29<00:21, 8114.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▊    | 242404/414113 [00:29<00:21, 8113.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▊    | 243215/414113 [00:29<00:21, 8112.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 244062/414113 [00:30<00:20, 8113.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 244901/414113 [00:30<00:20, 8114.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 245733/414113 [00:30<00:20, 8115.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 246559/414113 [00:30<00:20, 8115.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 247406/414113 [00:30<00:20, 8116.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 248239/414113 [00:30<00:20, 8117.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 249070/414113 [00:30<00:20, 8117.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 249898/414113 [00:30<00:20, 8117.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 250722/414113 [00:30<00:20, 8117.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 251543/414113 [00:30<00:20, 8114.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 252431/414113 [00:31<00:19, 8116.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 253294/414113 [00:31<00:19, 8118.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████▏   | 254128/414113 [00:31<00:19, 8118.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 254958/414113 [00:31<00:19, 8119.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 255805/414113 [00:31<00:19, 8120.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 256651/414113 [00:31<00:19, 8121.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 257496/414113 [00:31<00:19, 8122.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 258351/414113 [00:31<00:19, 8123.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 259195/414113 [00:31<00:19, 8123.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 260055/414113 [00:32<00:18, 8125.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 260922/414113 [00:32<00:18, 8127.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 261774/414113 [00:32<00:18, 8128.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 262625/414113 [00:32<00:18, 8129.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▎   | 263510/414113 [00:32<00:18, 8131.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 264370/414113 [00:32<00:18, 8132.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 265229/414113 [00:32<00:18, 8133.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 266086/414113 [00:32<00:18, 8133.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 266929/414113 [00:32<00:18, 8133.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 267763/414113 [00:32<00:17, 8133.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 268598/414113 [00:33<00:17, 8134.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 269426/414113 [00:33<00:17, 8133.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 270249/414113 [00:33<00:17, 8133.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 271068/414113 [00:33<00:17, 8133.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 271898/414113 [00:33<00:17, 8133.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 272723/414113 [00:33<00:17, 8134.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 273546/414113 [00:33<00:17, 8134.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▋   | 274386/414113 [00:33<00:17, 8135.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▋   | 275213/414113 [00:33<00:17, 8134.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 276053/414113 [00:33<00:16, 8135.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 276888/414113 [00:34<00:16, 8135.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 277715/414113 [00:34<00:16, 8136.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 278542/414113 [00:34<00:16, 8136.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 279383/414113 [00:34<00:16, 8137.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 280214/414113 [00:34<00:16, 8137.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 281043/414113 [00:34<00:16, 8136.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 281875/414113 [00:34<00:16, 8136.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 282715/414113 [00:34<00:16, 8137.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 283540/414113 [00:34<00:16, 8136.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▊   | 284406/414113 [00:34<00:15, 8138.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 285243/414113 [00:35<00:15, 8138.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 286074/414113 [00:35<00:15, 8138.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 286903/414113 [00:35<00:15, 8138.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 287765/414113 [00:35<00:15, 8140.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 288614/414113 [00:35<00:15, 8141.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 289470/414113 [00:35<00:15, 8142.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 290347/414113 [00:35<00:15, 8144.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 291201/414113 [00:35<00:15, 8144.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 292045/414113 [00:35<00:14, 8144.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 292904/414113 [00:35<00:14, 8145.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 293746/414113 [00:36<00:14, 8076.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 294549/414113 [00:36<00:14, 8076.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████▏  | 295423/414113 [00:36<00:14, 8078.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 296304/414113 [00:36<00:14, 8080.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 297192/414113 [00:36<00:14, 8082.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 298049/414113 [00:36<00:14, 8084.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 298922/414113 [00:36<00:14, 8085.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 299797/414113 [00:37<00:14, 8087.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 300664/414113 [00:37<00:14, 8089.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 301536/414113 [00:37<00:13, 8090.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 302421/414113 [00:37<00:13, 8092.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 303300/414113 [00:37<00:13, 8094.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 304176/414113 [00:37<00:13, 8096.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▎  | 305049/414113 [00:37<00:13, 8097.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 305920/414113 [00:37<00:13, 8099.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 306795/414113 [00:37<00:13, 8101.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 307667/414113 [00:37<00:13, 8102.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▍  | 308544/414113 [00:38<00:13, 8104.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▍  | 309417/414113 [00:38<00:12, 8106.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▍  | 310289/414113 [00:38<00:12, 8107.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 311159/414113 [00:38<00:12, 8108.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 312048/414113 [00:38<00:12, 8110.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 312941/414113 [00:38<00:12, 8112.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 313820/414113 [00:38<00:12, 8114.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 314707/414113 [00:38<00:12, 8116.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 315589/414113 [00:38<00:12, 8116.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▋  | 316465/414113 [00:38<00:12, 8118.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 317365/414113 [00:39<00:11, 8120.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 318241/414113 [00:39<00:11, 8121.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 319120/414113 [00:39<00:11, 8123.48it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 319994/414113 [00:39<00:11, 8125.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 320891/414113 [00:39<00:11, 8127.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 321798/414113 [00:39<00:11, 8129.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 322687/414113 [00:39<00:11, 8131.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 323574/414113 [00:39<00:11, 8132.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 324449/414113 [00:39<00:11, 8133.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▊  | 325320/414113 [00:39<00:10, 8133.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 326180/414113 [00:40<00:10, 8135.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 327039/414113 [00:40<00:10, 8135.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 327889/414113 [00:40<00:10, 8135.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 328741/414113 [00:40<00:10, 8136.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████▉  | 329591/414113 [00:40<00:10, 8137.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████▉  | 330436/414113 [00:40<00:10, 8137.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████▉  | 331273/414113 [00:40<00:10, 8137.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 332119/414113 [00:40<00:10, 8138.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 332988/414113 [00:40<00:09, 8139.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 333842/414113 [00:41<00:09, 8140.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 334691/414113 [00:41<00:09, 8140.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 335528/414113 [00:41<00:09, 8140.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 336360/414113 [00:41<00:09, 8139.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████▏ | 337170/414113 [00:41<00:09, 8138.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 337987/414113 [00:41<00:09, 8138.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 338802/414113 [00:41<00:09, 8138.57it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 339628/414113 [00:41<00:09, 8138.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 340442/414113 [00:41<00:09, 8138.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 341268/414113 [00:41<00:08, 8138.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 342084/414113 [00:42<00:08, 8138.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 342894/414113 [00:42<00:08, 8136.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 343726/414113 [00:42<00:08, 8137.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 344537/414113 [00:42<00:08, 8136.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 345360/414113 [00:42<00:08, 8137.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▎ | 346202/414113 [00:42<00:08, 8137.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 347024/414113 [00:42<00:08, 8138.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 347858/414113 [00:42<00:08, 8138.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 348682/414113 [00:42<00:08, 8138.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 349532/414113 [00:42<00:07, 8139.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 350379/414113 [00:43<00:07, 8140.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 351215/414113 [00:43<00:07, 8140.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 352049/414113 [00:43<00:07, 8140.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 352890/414113 [00:43<00:07, 8141.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 353725/414113 [00:43<00:07, 8142.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 354560/414113 [00:43<00:07, 8142.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 355396/414113 [00:43<00:07, 8142.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 356240/414113 [00:43<00:07, 8143.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 357077/414113 [00:43<00:07, 8143.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▋ | 357930/414113 [00:43<00:06, 8144.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 358775/414113 [00:44<00:06, 8145.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 359616/414113 [00:44<00:06, 8145.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 360454/414113 [00:44<00:06, 8145.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 361288/414113 [00:44<00:06, 8145.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 362119/414113 [00:44<00:06, 8146.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 362949/414113 [00:44<00:06, 8146.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 363777/414113 [00:44<00:06, 8146.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 364600/414113 [00:44<00:06, 8145.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 365443/414113 [00:44<00:05, 8146.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 366267/414113 [00:44<00:05, 8146.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▊ | 367088/414113 [00:45<00:05, 8146.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 367908/414113 [00:45<00:05, 8146.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 368727/414113 [00:45<00:05, 8145.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 369552/414113 [00:45<00:05, 8146.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 370367/414113 [00:45<00:05, 8146.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████▉ | 371203/414113 [00:45<00:05, 8146.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████▉ | 372023/414113 [00:45<00:05, 8146.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 372856/414113 [00:45<00:05, 8146.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 373677/414113 [00:45<00:04, 8146.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 374500/414113 [00:45<00:04, 8146.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 375316/414113 [00:46<00:04, 8146.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 376128/414113 [00:46<00:04, 8144.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 376954/414113 [00:46<00:04, 8145.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 377781/414113 [00:46<00:04, 8145.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████▏| 378662/414113 [00:46<00:04, 8146.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 379541/414113 [00:46<00:04, 8148.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 380393/414113 [00:46<00:04, 8148.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 381241/414113 [00:46<00:04, 8148.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 382081/414113 [00:46<00:03, 8149.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 382918/414113 [00:46<00:03, 8149.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 383750/414113 [00:47<00:03, 8149.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 384590/414113 [00:47<00:03, 8150.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 385423/414113 [00:47<00:03, 8150.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 386252/414113 [00:47<00:03, 8150.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 387106/414113 [00:47<00:03, 8151.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▎| 387939/414113 [00:47<00:03, 8150.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 388755/414113 [00:47<00:03, 8150.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 389596/414113 [00:47<00:03, 8150.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 390419/414113 [00:47<00:02, 8150.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 391276/414113 [00:48<00:02, 8150.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▍| 392148/414113 [00:48<00:02, 8152.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▍| 393009/414113 [00:48<00:02, 8153.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 393870/414113 [00:48<00:02, 8154.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 394721/414113 [00:48<00:02, 8152.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 395548/414113 [00:48<00:02, 8151.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 396363/414113 [00:48<00:02, 8151.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 397174/414113 [00:48<00:02, 8151.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 397983/414113 [00:48<00:01, 8150.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▋| 398784/414113 [00:48<00:01, 8150.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▋| 399590/414113 [00:49<00:01, 8150.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 400432/414113 [00:49<00:01, 8150.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 401243/414113 [00:49<00:01, 8150.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 402121/414113 [00:49<00:01, 8151.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 402949/414113 [00:49<00:01, 8150.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 403766/414113 [00:49<00:01, 8150.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 404642/414113 [00:49<00:01, 8151.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 405474/414113 [00:49<00:01, 8150.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 406285/414113 [00:49<00:00, 8150.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 407108/414113 [00:49<00:00, 8150.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▊| 407938/414113 [00:50<00:00, 8150.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▊| 408754/414113 [00:50<00:00, 8150.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 409568/414113 [00:50<00:00, 8150.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 410377/414113 [00:50<00:00, 8149.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 411186/414113 [00:50<00:00, 8149.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 412014/414113 [00:50<00:00, 8149.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|█████████▉| 412832/414113 [00:50<00:00, 8149.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|█████████▉| 413666/414113 [00:50<00:00, 8149.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 414113/414113 [00:50<00:00, 8148.63it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderCNN(\n",
      "  (resnet): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (6): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (7): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (8): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (9): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (10): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (11): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (12): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (13): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (14): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (15): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (16): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (17): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (18): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (19): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (20): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (21): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (22): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (8): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  )\n",
      "  (embed): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n",
      "DecoderRNN(\n",
      "  (embedding): Embedding(9956, 512)\n",
      "  (lstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=9956, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 16          # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    # transforms.RandomRotation(10),                   # get rotation image with random degree\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers=2)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = optim.Adam(params, lr = 0.0001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)\n",
    "\n",
    "# print model\n",
    "print(encoder)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/25883], Loss: 5.2812, Perplexity: 196.6101\n",
      "Epoch [1/3], Step [200/25883], Loss: 4.8481, Perplexity: 127.4955\n",
      "Epoch [1/3], Step [300/25883], Loss: 4.6740, Perplexity: 107.1265\n",
      "Epoch [1/3], Step [400/25883], Loss: 4.5426, Perplexity: 93.93216\n",
      "Epoch [1/3], Step [500/25883], Loss: 4.4909, Perplexity: 89.19793\n",
      "Epoch [1/3], Step [600/25883], Loss: 4.5164, Perplexity: 91.50289\n",
      "Epoch [1/3], Step [700/25883], Loss: 4.0797, Perplexity: 59.12980\n",
      "Epoch [1/3], Step [800/25883], Loss: 4.2317, Perplexity: 68.83623\n",
      "Epoch [1/3], Step [900/25883], Loss: 4.1595, Perplexity: 64.03875\n",
      "Epoch [1/3], Step [1000/25883], Loss: 4.3620, Perplexity: 78.4174\n",
      "Epoch [1/3], Step [1100/25883], Loss: 4.2236, Perplexity: 68.27603\n",
      "Epoch [1/3], Step [1200/25883], Loss: 4.3322, Perplexity: 76.11134\n",
      "Epoch [1/3], Step [1300/25883], Loss: 4.0815, Perplexity: 59.23150\n",
      "Epoch [1/3], Step [1400/25883], Loss: 4.0445, Perplexity: 57.08368\n",
      "Epoch [1/3], Step [1500/25883], Loss: 3.6502, Perplexity: 38.48156\n",
      "Epoch [1/3], Step [1600/25883], Loss: 4.7574, Perplexity: 116.4408\n",
      "Epoch [1/3], Step [1700/25883], Loss: 3.5262, Perplexity: 33.99530\n",
      "Epoch [1/3], Step [1800/25883], Loss: 3.2949, Perplexity: 26.97555\n",
      "Epoch [1/3], Step [1900/25883], Loss: 3.7852, Perplexity: 44.0466\n",
      "Epoch [1/3], Step [2000/25883], Loss: 3.4191, Perplexity: 30.54225\n",
      "Epoch [1/3], Step [2100/25883], Loss: 3.3085, Perplexity: 27.34510\n",
      "Epoch [1/3], Step [2200/25883], Loss: 3.5954, Perplexity: 36.4305\n",
      "Epoch [1/3], Step [2300/25883], Loss: 4.1621, Perplexity: 64.20691\n",
      "Epoch [1/3], Step [2400/25883], Loss: 3.7380, Perplexity: 42.0143\n",
      "Epoch [1/3], Step [2500/25883], Loss: 3.7326, Perplexity: 41.78577\n",
      "Epoch [1/3], Step [2600/25883], Loss: 3.6982, Perplexity: 40.37468\n",
      "Epoch [1/3], Step [2700/25883], Loss: 3.2593, Perplexity: 26.03073\n",
      "Epoch [1/3], Step [2800/25883], Loss: 3.3322, Perplexity: 28.0011\n",
      "Epoch [1/3], Step [2900/25883], Loss: 3.7410, Perplexity: 42.14021\n",
      "Epoch [1/3], Step [3000/25883], Loss: 3.2927, Perplexity: 26.9154\n",
      "Epoch [1/3], Step [3100/25883], Loss: 3.6395, Perplexity: 38.0725\n",
      "Epoch [1/3], Step [3200/25883], Loss: 3.6664, Perplexity: 39.11216\n",
      "Epoch [1/3], Step [3300/25883], Loss: 3.3396, Perplexity: 28.2090\n",
      "Epoch [1/3], Step [3400/25883], Loss: 3.6949, Perplexity: 40.2419\n",
      "Epoch [1/3], Step [3500/25883], Loss: 4.0421, Perplexity: 56.9456\n",
      "Epoch [1/3], Step [3600/25883], Loss: 3.0809, Perplexity: 21.7778\n",
      "Epoch [1/3], Step [3700/25883], Loss: 3.8078, Perplexity: 45.0528\n",
      "Epoch [1/3], Step [3800/25883], Loss: 3.2788, Perplexity: 26.5445\n",
      "Epoch [1/3], Step [3900/25883], Loss: 3.0858, Perplexity: 21.8840\n",
      "Epoch [1/3], Step [4000/25883], Loss: 3.4025, Perplexity: 30.0377\n",
      "Epoch [1/3], Step [4100/25883], Loss: 3.5089, Perplexity: 33.41057\n",
      "Epoch [1/3], Step [4200/25883], Loss: 3.3203, Perplexity: 27.6697\n",
      "Epoch [1/3], Step [4300/25883], Loss: 2.8691, Perplexity: 17.62124\n",
      "Epoch [1/3], Step [4400/25883], Loss: 4.1191, Perplexity: 61.5053\n",
      "Epoch [1/3], Step [4500/25883], Loss: 3.1903, Perplexity: 24.2966\n",
      "Epoch [1/3], Step [4600/25883], Loss: 3.2774, Perplexity: 26.50783\n",
      "Epoch [1/3], Step [4700/25883], Loss: 3.4876, Perplexity: 32.7074\n",
      "Epoch [1/3], Step [4800/25883], Loss: 3.1056, Perplexity: 22.3233\n",
      "Epoch [1/3], Step [4900/25883], Loss: 3.3894, Perplexity: 29.6471\n",
      "Epoch [1/3], Step [5000/25883], Loss: 3.1475, Perplexity: 23.2778\n",
      "Epoch [1/3], Step [5100/25883], Loss: 3.6306, Perplexity: 37.7372\n",
      "Epoch [1/3], Step [5200/25883], Loss: 3.2651, Perplexity: 26.1831\n",
      "Epoch [1/3], Step [5300/25883], Loss: 3.0723, Perplexity: 21.59238\n",
      "Epoch [1/3], Step [5400/25883], Loss: 2.6195, Perplexity: 13.7293\n",
      "Epoch [1/3], Step [5500/25883], Loss: 2.5564, Perplexity: 12.8891\n",
      "Epoch [1/3], Step [5600/25883], Loss: 2.9993, Perplexity: 20.0705\n",
      "Epoch [1/3], Step [5700/25883], Loss: 3.0121, Perplexity: 20.3298\n",
      "Epoch [1/3], Step [5800/25883], Loss: 3.0734, Perplexity: 21.6162\n",
      "Epoch [1/3], Step [5900/25883], Loss: 3.2841, Perplexity: 26.6837\n",
      "Epoch [1/3], Step [6000/25883], Loss: 3.4382, Perplexity: 31.1317\n",
      "Epoch [1/3], Step [6100/25883], Loss: 2.6105, Perplexity: 13.6062\n",
      "Epoch [1/3], Step [6200/25883], Loss: 2.8929, Perplexity: 18.0463\n",
      "Epoch [1/3], Step [6300/25883], Loss: 3.0735, Perplexity: 21.6184\n",
      "Epoch [1/3], Step [6400/25883], Loss: 2.7231, Perplexity: 15.2280\n",
      "Epoch [1/3], Step [6500/25883], Loss: 3.9365, Perplexity: 51.2389\n",
      "Epoch [1/3], Step [6600/25883], Loss: 2.9438, Perplexity: 18.9876\n",
      "Epoch [1/3], Step [6700/25883], Loss: 3.3502, Perplexity: 28.5097\n",
      "Epoch [1/3], Step [6800/25883], Loss: 3.6512, Perplexity: 38.52169\n",
      "Epoch [1/3], Step [6900/25883], Loss: 3.0629, Perplexity: 21.3902\n",
      "Epoch [1/3], Step [7000/25883], Loss: 3.1731, Perplexity: 23.88105\n",
      "Epoch [1/3], Step [7100/25883], Loss: 2.8731, Perplexity: 17.6922\n",
      "Epoch [1/3], Step [7200/25883], Loss: 3.5415, Perplexity: 34.51793\n",
      "Epoch [1/3], Step [7300/25883], Loss: 3.0900, Perplexity: 21.9766\n",
      "Epoch [1/3], Step [7400/25883], Loss: 2.8957, Perplexity: 18.0963\n",
      "Epoch [1/3], Step [7500/25883], Loss: 3.1521, Perplexity: 23.3846\n",
      "Epoch [1/3], Step [7600/25883], Loss: 3.2779, Perplexity: 26.5198\n",
      "Epoch [1/3], Step [7700/25883], Loss: 2.9243, Perplexity: 18.6212\n",
      "Epoch [1/3], Step [7800/25883], Loss: 2.4270, Perplexity: 11.3246\n",
      "Epoch [1/3], Step [7900/25883], Loss: 2.5668, Perplexity: 13.0241\n",
      "Epoch [1/3], Step [8000/25883], Loss: 2.9982, Perplexity: 20.0503\n",
      "Epoch [1/3], Step [8100/25883], Loss: 2.7899, Perplexity: 16.2795\n",
      "Epoch [1/3], Step [8200/25883], Loss: 3.0594, Perplexity: 21.3142\n",
      "Epoch [1/3], Step [8300/25883], Loss: 3.0516, Perplexity: 21.1485\n",
      "Epoch [1/3], Step [8400/25883], Loss: 2.6599, Perplexity: 14.2947\n",
      "Epoch [1/3], Step [8500/25883], Loss: 2.6648, Perplexity: 14.3647\n",
      "Epoch [1/3], Step [8600/25883], Loss: 3.0221, Perplexity: 20.5339\n",
      "Epoch [1/3], Step [8700/25883], Loss: 2.9192, Perplexity: 18.5256\n",
      "Epoch [1/3], Step [8800/25883], Loss: 2.8110, Perplexity: 16.6262\n",
      "Epoch [1/3], Step [8900/25883], Loss: 2.8580, Perplexity: 17.42695\n",
      "Epoch [1/3], Step [9000/25883], Loss: 2.7898, Perplexity: 16.2775\n",
      "Epoch [1/3], Step [9100/25883], Loss: 2.6809, Perplexity: 14.5975\n",
      "Epoch [1/3], Step [9200/25883], Loss: 3.2221, Perplexity: 25.0818\n",
      "Epoch [1/3], Step [9300/25883], Loss: 3.0236, Perplexity: 20.5654\n",
      "Epoch [1/3], Step [9400/25883], Loss: 2.7771, Perplexity: 16.0723\n",
      "Epoch [1/3], Step [9500/25883], Loss: 2.6111, Perplexity: 13.6137\n",
      "Epoch [1/3], Step [9600/25883], Loss: 2.5896, Perplexity: 13.3238\n",
      "Epoch [1/3], Step [9700/25883], Loss: 3.0360, Perplexity: 20.8217\n",
      "Epoch [1/3], Step [9800/25883], Loss: 2.8303, Perplexity: 16.9505\n",
      "Epoch [1/3], Step [9900/25883], Loss: 3.4134, Perplexity: 30.3691\n",
      "Epoch [1/3], Step [10000/25883], Loss: 3.0924, Perplexity: 22.0304\n",
      "Epoch [1/3], Step [10100/25883], Loss: 3.0246, Perplexity: 20.5852\n",
      "Epoch [1/3], Step [10200/25883], Loss: 3.1264, Perplexity: 22.7914\n",
      "Epoch [1/3], Step [10300/25883], Loss: 2.9396, Perplexity: 18.9088\n",
      "Epoch [1/3], Step [10400/25883], Loss: 2.9139, Perplexity: 18.4280\n",
      "Epoch [1/3], Step [10500/25883], Loss: 2.6284, Perplexity: 13.8515\n",
      "Epoch [1/3], Step [10600/25883], Loss: 2.3225, Perplexity: 10.2012\n",
      "Epoch [1/3], Step [10700/25883], Loss: 2.7025, Perplexity: 14.9174\n",
      "Epoch [1/3], Step [10800/25883], Loss: 2.5186, Perplexity: 12.4114\n",
      "Epoch [1/3], Step [10900/25883], Loss: 2.5463, Perplexity: 12.7598\n",
      "Epoch [1/3], Step [11000/25883], Loss: 3.1272, Perplexity: 22.8099\n",
      "Epoch [1/3], Step [11100/25883], Loss: 2.6018, Perplexity: 13.4885\n",
      "Epoch [1/3], Step [11200/25883], Loss: 2.7674, Perplexity: 15.9169\n",
      "Epoch [1/3], Step [11300/25883], Loss: 3.0026, Perplexity: 20.1385\n",
      "Epoch [1/3], Step [11400/25883], Loss: 2.0215, Perplexity: 7.54980\n",
      "Epoch [1/3], Step [11500/25883], Loss: 3.0295, Perplexity: 20.6862\n",
      "Epoch [1/3], Step [11600/25883], Loss: 2.4588, Perplexity: 11.6909\n",
      "Epoch [1/3], Step [11700/25883], Loss: 2.5301, Perplexity: 12.55495\n",
      "Epoch [1/3], Step [11800/25883], Loss: 3.8294, Perplexity: 46.0342\n",
      "Epoch [1/3], Step [11900/25883], Loss: 2.4119, Perplexity: 11.1556\n",
      "Epoch [1/3], Step [12000/25883], Loss: 3.1825, Perplexity: 24.1069\n",
      "Epoch [1/3], Step [12100/25883], Loss: 2.3598, Perplexity: 10.5890\n",
      "Epoch [1/3], Step [12200/25883], Loss: 2.5737, Perplexity: 13.1148\n",
      "Epoch [1/3], Step [12300/25883], Loss: 2.5299, Perplexity: 12.5520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [12400/25883], Loss: 3.0889, Perplexity: 21.9538\n",
      "Epoch [1/3], Step [12500/25883], Loss: 3.1876, Perplexity: 24.2308\n",
      "Epoch [1/3], Step [12600/25883], Loss: 2.2407, Perplexity: 9.40038\n",
      "Epoch [1/3], Step [12700/25883], Loss: 2.3447, Perplexity: 10.4300\n",
      "Epoch [1/3], Step [12800/25883], Loss: 2.7288, Perplexity: 15.3139\n",
      "Epoch [1/3], Step [12900/25883], Loss: 2.6298, Perplexity: 13.87089\n",
      "Epoch [1/3], Step [13000/25883], Loss: 2.6733, Perplexity: 14.4878\n",
      "Epoch [1/3], Step [13100/25883], Loss: 2.3989, Perplexity: 11.0107\n",
      "Epoch [1/3], Step [13200/25883], Loss: 2.7169, Perplexity: 15.1333\n",
      "Epoch [1/3], Step [13300/25883], Loss: 2.7072, Perplexity: 14.9867\n",
      "Epoch [1/3], Step [13400/25883], Loss: 2.7186, Perplexity: 15.1586\n",
      "Epoch [1/3], Step [13500/25883], Loss: 2.4958, Perplexity: 12.1312\n",
      "Epoch [1/3], Step [13600/25883], Loss: 2.7903, Perplexity: 16.2857\n",
      "Epoch [1/3], Step [13700/25883], Loss: 2.9079, Perplexity: 18.3179\n",
      "Epoch [1/3], Step [13800/25883], Loss: 2.6367, Perplexity: 13.9673\n",
      "Epoch [1/3], Step [13900/25883], Loss: 2.3120, Perplexity: 10.0943\n",
      "Epoch [1/3], Step [14000/25883], Loss: 2.3268, Perplexity: 10.2452\n",
      "Epoch [1/3], Step [14100/25883], Loss: 2.7800, Perplexity: 16.1187\n",
      "Epoch [1/3], Step [14200/25883], Loss: 2.5090, Perplexity: 12.2926\n",
      "Epoch [1/3], Step [14300/25883], Loss: 3.2054, Perplexity: 24.6644\n",
      "Epoch [1/3], Step [14400/25883], Loss: 2.4893, Perplexity: 12.0528\n",
      "Epoch [1/3], Step [14500/25883], Loss: 2.3834, Perplexity: 10.8414\n",
      "Epoch [1/3], Step [14600/25883], Loss: 2.3974, Perplexity: 10.9942\n",
      "Epoch [1/3], Step [14700/25883], Loss: 2.3945, Perplexity: 10.9631\n",
      "Epoch [1/3], Step [14800/25883], Loss: 3.2151, Perplexity: 24.9070\n",
      "Epoch [1/3], Step [14900/25883], Loss: 2.4362, Perplexity: 11.4293\n",
      "Epoch [1/3], Step [15000/25883], Loss: 2.4452, Perplexity: 11.5334\n",
      "Epoch [1/3], Step [15100/25883], Loss: 2.8368, Perplexity: 17.0605\n",
      "Epoch [1/3], Step [15200/25883], Loss: 2.4606, Perplexity: 11.7119\n",
      "Epoch [1/3], Step [15300/25883], Loss: 2.6628, Perplexity: 14.3358\n",
      "Epoch [1/3], Step [15400/25883], Loss: 2.3798, Perplexity: 10.8024\n",
      "Epoch [1/3], Step [15500/25883], Loss: 2.6509, Perplexity: 14.1673\n",
      "Epoch [1/3], Step [15600/25883], Loss: 2.7300, Perplexity: 15.3333\n",
      "Epoch [1/3], Step [15700/25883], Loss: 3.1709, Perplexity: 23.8279\n",
      "Epoch [1/3], Step [15800/25883], Loss: 2.3111, Perplexity: 10.0850\n",
      "Epoch [1/3], Step [15900/25883], Loss: 2.4767, Perplexity: 11.9015\n",
      "Epoch [1/3], Step [16000/25883], Loss: 2.8305, Perplexity: 16.9544\n",
      "Epoch [1/3], Step [16100/25883], Loss: 2.6645, Perplexity: 14.3605\n",
      "Epoch [1/3], Step [16200/25883], Loss: 3.4615, Perplexity: 31.8661\n",
      "Epoch [1/3], Step [16300/25883], Loss: 2.6226, Perplexity: 13.7711\n",
      "Epoch [1/3], Step [16400/25883], Loss: 2.3963, Perplexity: 10.9826\n",
      "Epoch [1/3], Step [16500/25883], Loss: 2.9316, Perplexity: 18.75797\n",
      "Epoch [1/3], Step [16600/25883], Loss: 2.4488, Perplexity: 11.5740\n",
      "Epoch [1/3], Step [16700/25883], Loss: 2.7348, Perplexity: 15.4068\n",
      "Epoch [1/3], Step [16800/25883], Loss: 2.7460, Perplexity: 15.5805\n",
      "Epoch [1/3], Step [16900/25883], Loss: 2.4487, Perplexity: 11.5737\n",
      "Epoch [1/3], Step [17000/25883], Loss: 2.7329, Perplexity: 15.3773\n",
      "Epoch [1/3], Step [17100/25883], Loss: 2.8472, Perplexity: 17.2400\n",
      "Epoch [1/3], Step [17200/25883], Loss: 2.9394, Perplexity: 18.9038\n",
      "Epoch [1/3], Step [17300/25883], Loss: 2.8333, Perplexity: 17.0012\n",
      "Epoch [1/3], Step [17400/25883], Loss: 2.5624, Perplexity: 12.9669\n",
      "Epoch [1/3], Step [17500/25883], Loss: 2.9187, Perplexity: 18.5171\n",
      "Epoch [1/3], Step [17600/25883], Loss: 2.5712, Perplexity: 13.08120\n",
      "Epoch [1/3], Step [17700/25883], Loss: 2.5262, Perplexity: 12.5060\n",
      "Epoch [1/3], Step [17800/25883], Loss: 2.3039, Perplexity: 10.0133\n",
      "Epoch [1/3], Step [17900/25883], Loss: 2.2194, Perplexity: 9.20226\n",
      "Epoch [1/3], Step [18000/25883], Loss: 2.5631, Perplexity: 12.9758\n",
      "Epoch [1/3], Step [18100/25883], Loss: 2.7284, Perplexity: 15.3081\n",
      "Epoch [1/3], Step [18200/25883], Loss: 2.8228, Perplexity: 16.8233\n",
      "Epoch [1/3], Step [18300/25883], Loss: 2.5728, Perplexity: 13.1024\n",
      "Epoch [1/3], Step [18400/25883], Loss: 2.2381, Perplexity: 9.37515\n",
      "Epoch [1/3], Step [18500/25883], Loss: 2.3140, Perplexity: 10.1145\n",
      "Epoch [1/3], Step [18600/25883], Loss: 3.1639, Perplexity: 23.6627\n",
      "Epoch [1/3], Step [18700/25883], Loss: 2.4430, Perplexity: 11.5071\n",
      "Epoch [1/3], Step [18800/25883], Loss: 2.6075, Perplexity: 13.5647\n",
      "Epoch [1/3], Step [18900/25883], Loss: 2.2604, Perplexity: 9.58653\n",
      "Epoch [1/3], Step [19000/25883], Loss: 3.2384, Perplexity: 25.4916\n",
      "Epoch [1/3], Step [19100/25883], Loss: 3.0068, Perplexity: 20.2227\n",
      "Epoch [1/3], Step [19200/25883], Loss: 2.3880, Perplexity: 10.8917\n",
      "Epoch [1/3], Step [19300/25883], Loss: 2.5511, Perplexity: 12.8218\n",
      "Epoch [1/3], Step [19400/25883], Loss: 2.3288, Perplexity: 10.2655\n",
      "Epoch [1/3], Step [19500/25883], Loss: 2.2031, Perplexity: 9.05332\n",
      "Epoch [1/3], Step [19600/25883], Loss: 2.4462, Perplexity: 11.5445\n",
      "Epoch [1/3], Step [19700/25883], Loss: 2.9374, Perplexity: 18.8660\n",
      "Epoch [1/3], Step [19800/25883], Loss: 2.4650, Perplexity: 11.7631\n",
      "Epoch [1/3], Step [19900/25883], Loss: 2.9960, Perplexity: 20.0045\n",
      "Epoch [1/3], Step [20000/25883], Loss: 2.6754, Perplexity: 14.5187\n",
      "Epoch [1/3], Step [20100/25883], Loss: 2.7224, Perplexity: 15.2166\n",
      "Epoch [1/3], Step [20200/25883], Loss: 2.2875, Perplexity: 9.85083\n",
      "Epoch [1/3], Step [20300/25883], Loss: 3.7552, Perplexity: 42.7408\n",
      "Epoch [1/3], Step [20400/25883], Loss: 2.7109, Perplexity: 15.0432\n",
      "Epoch [1/3], Step [20500/25883], Loss: 2.7026, Perplexity: 14.9186\n",
      "Epoch [1/3], Step [20600/25883], Loss: 2.4378, Perplexity: 11.4473\n",
      "Epoch [1/3], Step [20700/25883], Loss: 3.2494, Perplexity: 25.7758\n",
      "Epoch [1/3], Step [20800/25883], Loss: 2.3731, Perplexity: 10.7302\n",
      "Epoch [1/3], Step [20900/25883], Loss: 2.4263, Perplexity: 11.3174\n",
      "Epoch [1/3], Step [21000/25883], Loss: 2.5038, Perplexity: 12.2285\n",
      "Epoch [1/3], Step [21100/25883], Loss: 2.6059, Perplexity: 13.5434\n",
      "Epoch [1/3], Step [21200/25883], Loss: 2.9642, Perplexity: 19.3785\n",
      "Epoch [1/3], Step [21300/25883], Loss: 2.6111, Perplexity: 13.6135\n",
      "Epoch [1/3], Step [21400/25883], Loss: 2.7395, Perplexity: 15.4787\n",
      "Epoch [1/3], Step [21500/25883], Loss: 2.4270, Perplexity: 11.3246\n",
      "Epoch [1/3], Step [21600/25883], Loss: 2.4436, Perplexity: 11.5139\n",
      "Epoch [1/3], Step [21700/25883], Loss: 2.2570, Perplexity: 9.55453\n",
      "Epoch [1/3], Step [21800/25883], Loss: 3.3387, Perplexity: 28.1837\n",
      "Epoch [1/3], Step [21900/25883], Loss: 2.2117, Perplexity: 9.13150\n",
      "Epoch [1/3], Step [22000/25883], Loss: 2.6553, Perplexity: 14.2298\n",
      "Epoch [1/3], Step [22100/25883], Loss: 2.8315, Perplexity: 16.9704\n",
      "Epoch [1/3], Step [22200/25883], Loss: 2.4088, Perplexity: 11.1210\n",
      "Epoch [1/3], Step [22300/25883], Loss: 2.9998, Perplexity: 20.08075\n",
      "Epoch [1/3], Step [22400/25883], Loss: 2.3239, Perplexity: 10.2154\n",
      "Epoch [1/3], Step [22500/25883], Loss: 2.9040, Perplexity: 18.2474\n",
      "Epoch [1/3], Step [22600/25883], Loss: 2.6120, Perplexity: 13.6262\n",
      "Epoch [1/3], Step [22700/25883], Loss: 2.4552, Perplexity: 11.6488\n",
      "Epoch [1/3], Step [22800/25883], Loss: 2.7409, Perplexity: 15.5006\n",
      "Epoch [1/3], Step [22900/25883], Loss: 2.7768, Perplexity: 16.0679\n",
      "Epoch [1/3], Step [23000/25883], Loss: 2.4334, Perplexity: 11.3970\n",
      "Epoch [1/3], Step [23100/25883], Loss: 2.2391, Perplexity: 9.38515\n",
      "Epoch [1/3], Step [23200/25883], Loss: 2.7823, Perplexity: 16.1560\n",
      "Epoch [1/3], Step [23300/25883], Loss: 2.4654, Perplexity: 11.7678\n",
      "Epoch [1/3], Step [23400/25883], Loss: 2.8929, Perplexity: 18.0451\n",
      "Epoch [1/3], Step [23500/25883], Loss: 2.3658, Perplexity: 10.6527\n",
      "Epoch [1/3], Step [23600/25883], Loss: 2.3805, Perplexity: 10.8102\n",
      "Epoch [1/3], Step [23700/25883], Loss: 2.7116, Perplexity: 15.0535\n",
      "Epoch [1/3], Step [23800/25883], Loss: 2.7137, Perplexity: 15.0848\n",
      "Epoch [1/3], Step [23900/25883], Loss: 2.6806, Perplexity: 14.5938\n",
      "Epoch [1/3], Step [24000/25883], Loss: 2.5765, Perplexity: 13.1510\n",
      "Epoch [1/3], Step [24100/25883], Loss: 2.6522, Perplexity: 14.1856\n",
      "Epoch [1/3], Step [24200/25883], Loss: 2.8086, Perplexity: 16.5867\n",
      "Epoch [1/3], Step [24300/25883], Loss: 2.2299, Perplexity: 9.29933\n",
      "Epoch [1/3], Step [24400/25883], Loss: 2.5204, Perplexity: 12.4331\n",
      "Epoch [1/3], Step [24500/25883], Loss: 2.6600, Perplexity: 14.2961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [24600/25883], Loss: 2.4256, Perplexity: 11.3085\n",
      "Epoch [1/3], Step [24700/25883], Loss: 2.4829, Perplexity: 11.9754\n",
      "Epoch [1/3], Step [24800/25883], Loss: 2.7146, Perplexity: 15.0988\n",
      "Epoch [1/3], Step [24900/25883], Loss: 2.3137, Perplexity: 10.1118\n",
      "Epoch [1/3], Step [25000/25883], Loss: 2.3281, Perplexity: 10.2582\n",
      "Epoch [1/3], Step [25100/25883], Loss: 2.5263, Perplexity: 12.5069\n",
      "Epoch [1/3], Step [25200/25883], Loss: 2.2622, Perplexity: 9.60423\n",
      "Epoch [1/3], Step [25300/25883], Loss: 2.1959, Perplexity: 8.98853\n",
      "Epoch [1/3], Step [25400/25883], Loss: 2.8765, Perplexity: 17.7513\n",
      "Epoch [1/3], Step [25500/25883], Loss: 2.3938, Perplexity: 10.9554\n",
      "Epoch [1/3], Step [25600/25883], Loss: 2.8652, Perplexity: 17.5522\n",
      "Epoch [1/3], Step [25700/25883], Loss: 2.4292, Perplexity: 11.3496\n",
      "Epoch [1/3], Step [25800/25883], Loss: 2.6633, Perplexity: 14.3439\n",
      "Epoch [2/3], Step [100/25883], Loss: 2.1611, Perplexity: 8.6811185\n",
      "Epoch [2/3], Step [200/25883], Loss: 2.1801, Perplexity: 8.84695\n",
      "Epoch [2/3], Step [300/25883], Loss: 2.4876, Perplexity: 12.0321\n",
      "Epoch [2/3], Step [400/25883], Loss: 2.3167, Perplexity: 10.1420\n",
      "Epoch [2/3], Step [500/25883], Loss: 2.3471, Perplexity: 10.4556\n",
      "Epoch [2/3], Step [600/25883], Loss: 2.2388, Perplexity: 9.38218\n",
      "Epoch [2/3], Step [700/25883], Loss: 2.4523, Perplexity: 11.6148\n",
      "Epoch [2/3], Step [800/25883], Loss: 2.6222, Perplexity: 13.7656\n",
      "Epoch [2/3], Step [900/25883], Loss: 2.3121, Perplexity: 10.0954\n",
      "Epoch [2/3], Step [1000/25883], Loss: 2.9970, Perplexity: 20.0263\n",
      "Epoch [2/3], Step [1100/25883], Loss: 2.0877, Perplexity: 8.06675\n",
      "Epoch [2/3], Step [1200/25883], Loss: 2.5549, Perplexity: 12.8698\n",
      "Epoch [2/3], Step [1300/25883], Loss: 2.4050, Perplexity: 11.0780\n",
      "Epoch [2/3], Step [1400/25883], Loss: 2.3821, Perplexity: 10.8276\n",
      "Epoch [2/3], Step [1500/25883], Loss: 2.1785, Perplexity: 8.83288\n",
      "Epoch [2/3], Step [1600/25883], Loss: 2.7817, Perplexity: 16.1458\n",
      "Epoch [2/3], Step [1700/25883], Loss: 2.0283, Perplexity: 7.60081\n",
      "Epoch [2/3], Step [1800/25883], Loss: 3.2762, Perplexity: 26.4754\n",
      "Epoch [2/3], Step [1900/25883], Loss: 2.3927, Perplexity: 10.9429\n",
      "Epoch [2/3], Step [2000/25883], Loss: 2.4111, Perplexity: 11.1458\n",
      "Epoch [2/3], Step [2100/25883], Loss: 2.8386, Perplexity: 17.0916\n",
      "Epoch [2/3], Step [2200/25883], Loss: 2.4142, Perplexity: 11.1813\n",
      "Epoch [2/3], Step [2300/25883], Loss: 2.5371, Perplexity: 12.64281\n",
      "Epoch [2/3], Step [2400/25883], Loss: 2.4991, Perplexity: 12.1713\n",
      "Epoch [2/3], Step [2500/25883], Loss: 1.8591, Perplexity: 6.41782\n",
      "Epoch [2/3], Step [2600/25883], Loss: 2.3832, Perplexity: 10.8399\n",
      "Epoch [2/3], Step [2700/25883], Loss: 2.2290, Perplexity: 9.29063\n",
      "Epoch [2/3], Step [2800/25883], Loss: 1.9556, Perplexity: 7.06841\n",
      "Epoch [2/3], Step [2900/25883], Loss: 2.4231, Perplexity: 11.2802\n",
      "Epoch [2/3], Step [3000/25883], Loss: 2.4386, Perplexity: 11.4574\n",
      "Epoch [2/3], Step [3100/25883], Loss: 2.5592, Perplexity: 12.9259\n",
      "Epoch [2/3], Step [3200/25883], Loss: 3.2076, Perplexity: 24.7191\n",
      "Epoch [2/3], Step [3300/25883], Loss: 2.5109, Perplexity: 12.3159\n",
      "Epoch [2/3], Step [3400/25883], Loss: 2.5030, Perplexity: 12.2193\n",
      "Epoch [2/3], Step [3500/25883], Loss: 2.5462, Perplexity: 12.7582\n",
      "Epoch [2/3], Step [3600/25883], Loss: 2.1276, Perplexity: 8.39438\n",
      "Epoch [2/3], Step [3700/25883], Loss: 2.5789, Perplexity: 13.1825\n",
      "Epoch [2/3], Step [3800/25883], Loss: 2.0139, Perplexity: 7.49246\n",
      "Epoch [2/3], Step [3900/25883], Loss: 2.6277, Perplexity: 13.8415\n",
      "Epoch [2/3], Step [4000/25883], Loss: 2.6679, Perplexity: 14.4095\n",
      "Epoch [2/3], Step [4100/25883], Loss: 2.6686, Perplexity: 14.4191\n",
      "Epoch [2/3], Step [4200/25883], Loss: 3.1825, Perplexity: 24.1081\n",
      "Epoch [2/3], Step [4300/25883], Loss: 2.3630, Perplexity: 10.6227\n",
      "Epoch [2/3], Step [4400/25883], Loss: 2.5384, Perplexity: 12.6596\n",
      "Epoch [2/3], Step [4500/25883], Loss: 1.8599, Perplexity: 6.42338\n",
      "Epoch [2/3], Step [4600/25883], Loss: 2.6084, Perplexity: 13.5778\n",
      "Epoch [2/3], Step [4700/25883], Loss: 2.4126, Perplexity: 11.1626\n",
      "Epoch [2/3], Step [4800/25883], Loss: 2.2689, Perplexity: 9.66851\n",
      "Epoch [2/3], Step [4900/25883], Loss: 1.8994, Perplexity: 6.68185\n",
      "Epoch [2/3], Step [5000/25883], Loss: 2.0838, Perplexity: 8.03468\n",
      "Epoch [2/3], Step [5100/25883], Loss: 2.5733, Perplexity: 13.1089\n",
      "Epoch [2/3], Step [5200/25883], Loss: 2.5669, Perplexity: 13.02499\n",
      "Epoch [2/3], Step [5300/25883], Loss: 2.2429, Perplexity: 9.42107\n",
      "Epoch [2/3], Step [5400/25883], Loss: 2.2242, Perplexity: 9.24659\n",
      "Epoch [2/3], Step [5500/25883], Loss: 2.4205, Perplexity: 11.2514\n",
      "Epoch [2/3], Step [5600/25883], Loss: 2.6423, Perplexity: 14.0455\n",
      "Epoch [2/3], Step [5700/25883], Loss: 2.5993, Perplexity: 13.4549\n",
      "Epoch [2/3], Step [5800/25883], Loss: 2.5022, Perplexity: 12.2096\n",
      "Epoch [2/3], Step [5900/25883], Loss: 2.2063, Perplexity: 9.08244\n",
      "Epoch [2/3], Step [6000/25883], Loss: 2.6982, Perplexity: 14.8529\n",
      "Epoch [2/3], Step [6100/25883], Loss: 2.1208, Perplexity: 8.33773\n",
      "Epoch [2/3], Step [6200/25883], Loss: 2.2090, Perplexity: 9.10641\n",
      "Epoch [2/3], Step [6300/25883], Loss: 2.3560, Perplexity: 10.5483\n",
      "Epoch [2/3], Step [6400/25883], Loss: 2.1476, Perplexity: 8.56421\n",
      "Epoch [2/3], Step [6500/25883], Loss: 2.1228, Perplexity: 8.35457\n",
      "Epoch [2/3], Step [6600/25883], Loss: 2.1866, Perplexity: 8.90498\n",
      "Epoch [2/3], Step [6700/25883], Loss: 3.0852, Perplexity: 21.8716\n",
      "Epoch [2/3], Step [6800/25883], Loss: 2.4066, Perplexity: 11.0961\n",
      "Epoch [2/3], Step [6900/25883], Loss: 2.5835, Perplexity: 13.2433\n",
      "Epoch [2/3], Step [7000/25883], Loss: 2.3692, Perplexity: 10.6892\n",
      "Epoch [2/3], Step [7100/25883], Loss: 2.5982, Perplexity: 13.4394\n",
      "Epoch [2/3], Step [7200/25883], Loss: 2.5550, Perplexity: 12.8709\n",
      "Epoch [2/3], Step [7300/25883], Loss: 2.1619, Perplexity: 8.68795\n",
      "Epoch [2/3], Step [7400/25883], Loss: 2.7073, Perplexity: 14.9894\n",
      "Epoch [2/3], Step [7500/25883], Loss: 2.9755, Perplexity: 19.5988\n",
      "Epoch [2/3], Step [7600/25883], Loss: 3.1224, Perplexity: 22.7010\n",
      "Epoch [2/3], Step [7700/25883], Loss: 2.8855, Perplexity: 17.9125\n",
      "Epoch [2/3], Step [7800/25883], Loss: 2.7120, Perplexity: 15.0600\n",
      "Epoch [2/3], Step [7900/25883], Loss: 3.3211, Perplexity: 27.6917\n",
      "Epoch [2/3], Step [8000/25883], Loss: 2.6328, Perplexity: 13.9128\n",
      "Epoch [2/3], Step [8100/25883], Loss: 3.0748, Perplexity: 21.6461\n",
      "Epoch [2/3], Step [8200/25883], Loss: 2.0470, Perplexity: 7.74486\n",
      "Epoch [2/3], Step [8300/25883], Loss: 2.1115, Perplexity: 8.26097\n",
      "Epoch [2/3], Step [8400/25883], Loss: 2.4373, Perplexity: 11.4421\n",
      "Epoch [2/3], Step [8500/25883], Loss: 2.3179, Perplexity: 10.1548\n",
      "Epoch [2/3], Step [8600/25883], Loss: 2.4743, Perplexity: 11.8733\n",
      "Epoch [2/3], Step [8700/25883], Loss: 2.4885, Perplexity: 12.0428\n",
      "Epoch [2/3], Step [8800/25883], Loss: 2.2513, Perplexity: 9.50019\n",
      "Epoch [2/3], Step [8900/25883], Loss: 2.3995, Perplexity: 11.0181\n",
      "Epoch [2/3], Step [9000/25883], Loss: 2.6816, Perplexity: 14.6088\n",
      "Epoch [2/3], Step [9100/25883], Loss: 3.0830, Perplexity: 21.8227\n",
      "Epoch [2/3], Step [9200/25883], Loss: 2.7848, Perplexity: 16.1963\n",
      "Epoch [2/3], Step [9300/25883], Loss: 2.2166, Perplexity: 9.17651\n",
      "Epoch [2/3], Step [9400/25883], Loss: 2.3292, Perplexity: 10.2693\n",
      "Epoch [2/3], Step [9500/25883], Loss: 2.2644, Perplexity: 9.62575\n",
      "Epoch [2/3], Step [9600/25883], Loss: 2.1869, Perplexity: 8.90778\n",
      "Epoch [2/3], Step [9700/25883], Loss: 2.5525, Perplexity: 12.8391\n",
      "Epoch [2/3], Step [9800/25883], Loss: 2.0733, Perplexity: 7.95144\n",
      "Epoch [2/3], Step [9900/25883], Loss: 2.2606, Perplexity: 9.58909\n",
      "Epoch [2/3], Step [10000/25883], Loss: 2.3072, Perplexity: 10.0466\n",
      "Epoch [2/3], Step [10100/25883], Loss: 2.0448, Perplexity: 7.72794\n",
      "Epoch [2/3], Step [10200/25883], Loss: 2.1300, Perplexity: 8.41469\n",
      "Epoch [2/3], Step [10300/25883], Loss: 2.4031, Perplexity: 11.0579\n",
      "Epoch [2/3], Step [10400/25883], Loss: 2.3139, Perplexity: 10.1143\n",
      "Epoch [2/3], Step [10500/25883], Loss: 2.5979, Perplexity: 13.4353\n",
      "Epoch [2/3], Step [10600/25883], Loss: 2.1986, Perplexity: 9.01261\n",
      "Epoch [2/3], Step [10700/25883], Loss: 2.1590, Perplexity: 8.66258\n",
      "Epoch [2/3], Step [10800/25883], Loss: 2.4024, Perplexity: 11.0497\n",
      "Epoch [2/3], Step [10900/25883], Loss: 2.6178, Perplexity: 13.7050\n",
      "Epoch [2/3], Step [11000/25883], Loss: 2.7335, Perplexity: 15.3863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [11100/25883], Loss: 2.8221, Perplexity: 16.8117\n",
      "Epoch [2/3], Step [11200/25883], Loss: 2.4012, Perplexity: 11.0366\n",
      "Epoch [2/3], Step [11300/25883], Loss: 2.2784, Perplexity: 9.761027\n",
      "Epoch [2/3], Step [11400/25883], Loss: 2.4563, Perplexity: 11.6620\n",
      "Epoch [2/3], Step [11500/25883], Loss: 2.7774, Perplexity: 16.0767\n",
      "Epoch [2/3], Step [11600/25883], Loss: 2.6257, Perplexity: 13.8136\n",
      "Epoch [2/3], Step [11700/25883], Loss: 2.0088, Perplexity: 7.45421\n",
      "Epoch [2/3], Step [11800/25883], Loss: 2.5963, Perplexity: 13.4142\n",
      "Epoch [2/3], Step [11900/25883], Loss: 2.8233, Perplexity: 16.8317\n",
      "Epoch [2/3], Step [12000/25883], Loss: 2.2293, Perplexity: 9.29336\n",
      "Epoch [2/3], Step [12100/25883], Loss: 2.6193, Perplexity: 13.7262\n",
      "Epoch [2/3], Step [12200/25883], Loss: 2.0870, Perplexity: 8.06061\n",
      "Epoch [2/3], Step [12300/25883], Loss: 2.5591, Perplexity: 12.9237\n",
      "Epoch [2/3], Step [12400/25883], Loss: 2.3909, Perplexity: 10.9230\n",
      "Epoch [2/3], Step [12500/25883], Loss: 2.1495, Perplexity: 8.58053\n",
      "Epoch [2/3], Step [12600/25883], Loss: 1.9689, Perplexity: 7.16278\n",
      "Epoch [2/3], Step [12700/25883], Loss: 2.5854, Perplexity: 13.2688\n",
      "Epoch [2/3], Step [12800/25883], Loss: 2.7986, Perplexity: 16.4214\n",
      "Epoch [2/3], Step [12900/25883], Loss: 2.3309, Perplexity: 10.2875\n",
      "Epoch [2/3], Step [13000/25883], Loss: 2.4759, Perplexity: 11.8928\n",
      "Epoch [2/3], Step [13100/25883], Loss: 2.5644, Perplexity: 12.9925\n",
      "Epoch [2/3], Step [13200/25883], Loss: 2.1740, Perplexity: 8.79382\n",
      "Epoch [2/3], Step [13300/25883], Loss: 2.3944, Perplexity: 10.9616\n",
      "Epoch [2/3], Step [13400/25883], Loss: 2.1833, Perplexity: 8.87584\n",
      "Epoch [2/3], Step [13500/25883], Loss: 2.4280, Perplexity: 11.3359\n",
      "Epoch [2/3], Step [13600/25883], Loss: 1.8722, Perplexity: 6.50250\n",
      "Epoch [2/3], Step [13700/25883], Loss: 2.0226, Perplexity: 7.55836\n",
      "Epoch [2/3], Step [13800/25883], Loss: 2.1092, Perplexity: 8.24175\n",
      "Epoch [2/3], Step [13900/25883], Loss: 2.3622, Perplexity: 10.6140\n",
      "Epoch [2/3], Step [14000/25883], Loss: 2.7649, Perplexity: 15.8777\n",
      "Epoch [2/3], Step [14100/25883], Loss: 2.1635, Perplexity: 8.70133\n",
      "Epoch [2/3], Step [14200/25883], Loss: 2.2680, Perplexity: 9.659638\n",
      "Epoch [2/3], Step [14300/25883], Loss: 2.6315, Perplexity: 13.8952\n",
      "Epoch [2/3], Step [14400/25883], Loss: 2.7349, Perplexity: 15.4076\n",
      "Epoch [2/3], Step [14500/25883], Loss: 2.0343, Perplexity: 7.64673\n",
      "Epoch [2/3], Step [14600/25883], Loss: 2.2677, Perplexity: 9.65691\n",
      "Epoch [2/3], Step [14700/25883], Loss: 2.3287, Perplexity: 10.2649\n",
      "Epoch [2/3], Step [14800/25883], Loss: 2.1020, Perplexity: 8.18274\n",
      "Epoch [2/3], Step [14900/25883], Loss: 2.4087, Perplexity: 11.1192\n",
      "Epoch [2/3], Step [15000/25883], Loss: 2.2535, Perplexity: 9.52086\n",
      "Epoch [2/3], Step [15100/25883], Loss: 2.4644, Perplexity: 11.7560\n",
      "Epoch [2/3], Step [15200/25883], Loss: 2.3238, Perplexity: 10.2146\n",
      "Epoch [2/3], Step [15300/25883], Loss: 2.4618, Perplexity: 11.7254\n",
      "Epoch [2/3], Step [15400/25883], Loss: 2.5762, Perplexity: 13.1469\n",
      "Epoch [2/3], Step [15500/25883], Loss: 2.2750, Perplexity: 9.72775\n",
      "Epoch [2/3], Step [15600/25883], Loss: 2.1774, Perplexity: 8.82355\n",
      "Epoch [2/3], Step [15700/25883], Loss: 2.3533, Perplexity: 10.5202\n",
      "Epoch [2/3], Step [15800/25883], Loss: 3.5578, Perplexity: 35.0845\n",
      "Epoch [2/3], Step [15900/25883], Loss: 1.9836, Perplexity: 7.26910\n",
      "Epoch [2/3], Step [16000/25883], Loss: 2.4000, Perplexity: 11.0230\n",
      "Epoch [2/3], Step [16100/25883], Loss: 1.9209, Perplexity: 6.82736\n",
      "Epoch [2/3], Step [16200/25883], Loss: 2.6540, Perplexity: 14.2101\n",
      "Epoch [2/3], Step [16300/25883], Loss: 2.0638, Perplexity: 7.87556\n",
      "Epoch [2/3], Step [16400/25883], Loss: 2.7102, Perplexity: 15.0324\n",
      "Epoch [2/3], Step [16500/25883], Loss: 2.3738, Perplexity: 10.7380\n",
      "Epoch [2/3], Step [16600/25883], Loss: 2.2244, Perplexity: 9.24764\n",
      "Epoch [2/3], Step [16700/25883], Loss: 2.2662, Perplexity: 9.64285\n",
      "Epoch [2/3], Step [16800/25883], Loss: 2.6485, Perplexity: 14.1335\n",
      "Epoch [2/3], Step [16900/25883], Loss: 2.6221, Perplexity: 13.7646\n",
      "Epoch [2/3], Step [17000/25883], Loss: 2.3035, Perplexity: 10.0088\n",
      "Epoch [2/3], Step [17100/25883], Loss: 3.2812, Perplexity: 26.6074\n",
      "Epoch [2/3], Step [17200/25883], Loss: 2.1916, Perplexity: 8.949282\n",
      "Epoch [2/3], Step [17300/25883], Loss: 3.0439, Perplexity: 20.9861\n",
      "Epoch [2/3], Step [17400/25883], Loss: 2.3411, Perplexity: 10.3923\n",
      "Epoch [2/3], Step [17500/25883], Loss: 2.8157, Perplexity: 16.7048\n",
      "Epoch [2/3], Step [17600/25883], Loss: 2.5015, Perplexity: 12.2005\n",
      "Epoch [2/3], Step [17700/25883], Loss: 2.4953, Perplexity: 12.1259\n",
      "Epoch [2/3], Step [17800/25883], Loss: 1.8754, Perplexity: 6.52361\n",
      "Epoch [2/3], Step [17900/25883], Loss: 2.0959, Perplexity: 8.13283\n",
      "Epoch [2/3], Step [18000/25883], Loss: 2.0038, Perplexity: 7.41710\n",
      "Epoch [2/3], Step [18100/25883], Loss: 2.2940, Perplexity: 9.91449\n",
      "Epoch [2/3], Step [18200/25883], Loss: 2.2248, Perplexity: 9.25208\n",
      "Epoch [2/3], Step [18300/25883], Loss: 2.4216, Perplexity: 11.2639\n",
      "Epoch [2/3], Step [18400/25883], Loss: 2.6545, Perplexity: 14.2177\n",
      "Epoch [2/3], Step [18500/25883], Loss: 2.5759, Perplexity: 13.1430\n",
      "Epoch [2/3], Step [18600/25883], Loss: 2.1019, Perplexity: 8.18178\n",
      "Epoch [2/3], Step [18700/25883], Loss: 1.9654, Perplexity: 7.13770\n",
      "Epoch [2/3], Step [18800/25883], Loss: 2.3606, Perplexity: 10.5972\n",
      "Epoch [2/3], Step [18900/25883], Loss: 1.9882, Perplexity: 7.30246\n",
      "Epoch [2/3], Step [19000/25883], Loss: 2.3046, Perplexity: 10.0206\n",
      "Epoch [2/3], Step [19100/25883], Loss: 2.6212, Perplexity: 13.7528\n",
      "Epoch [2/3], Step [19200/25883], Loss: 2.2357, Perplexity: 9.35328\n",
      "Epoch [2/3], Step [19300/25883], Loss: 3.2552, Perplexity: 25.9249\n",
      "Epoch [2/3], Step [19400/25883], Loss: 2.3907, Perplexity: 10.9207\n",
      "Epoch [2/3], Step [19500/25883], Loss: 2.1092, Perplexity: 8.24135\n",
      "Epoch [2/3], Step [19600/25883], Loss: 2.3928, Perplexity: 10.9446\n",
      "Epoch [2/3], Step [19700/25883], Loss: 2.7969, Perplexity: 16.3935\n",
      "Epoch [2/3], Step [19800/25883], Loss: 2.0639, Perplexity: 7.87699\n",
      "Epoch [2/3], Step [19900/25883], Loss: 2.3474, Perplexity: 10.4581\n",
      "Epoch [2/3], Step [20000/25883], Loss: 2.3813, Perplexity: 10.8194\n",
      "Epoch [2/3], Step [20100/25883], Loss: 2.1716, Perplexity: 8.77268\n",
      "Epoch [2/3], Step [20200/25883], Loss: 2.3709, Perplexity: 10.7075\n",
      "Epoch [2/3], Step [20300/25883], Loss: 2.5449, Perplexity: 12.7415\n",
      "Epoch [2/3], Step [20400/25883], Loss: 2.1279, Perplexity: 8.39695\n",
      "Epoch [2/3], Step [20500/25883], Loss: 2.4601, Perplexity: 11.7059\n",
      "Epoch [2/3], Step [20600/25883], Loss: 2.3734, Perplexity: 10.7340\n",
      "Epoch [2/3], Step [20700/25883], Loss: 2.0777, Perplexity: 7.98578\n",
      "Epoch [2/3], Step [20800/25883], Loss: 2.2420, Perplexity: 9.412416\n",
      "Epoch [2/3], Step [20900/25883], Loss: 2.0568, Perplexity: 7.82087\n",
      "Epoch [2/3], Step [21000/25883], Loss: 2.4070, Perplexity: 11.1010\n",
      "Epoch [2/3], Step [21100/25883], Loss: 2.2180, Perplexity: 9.188961\n",
      "Epoch [2/3], Step [21200/25883], Loss: 2.1896, Perplexity: 8.93170\n",
      "Epoch [2/3], Step [21300/25883], Loss: 2.3779, Perplexity: 10.7821\n",
      "Epoch [2/3], Step [21400/25883], Loss: 2.3342, Perplexity: 10.3212\n",
      "Epoch [2/3], Step [21500/25883], Loss: 2.5876, Perplexity: 13.2983\n",
      "Epoch [2/3], Step [21600/25883], Loss: 2.4289, Perplexity: 11.3459\n",
      "Epoch [2/3], Step [21700/25883], Loss: 2.0879, Perplexity: 8.06820\n",
      "Epoch [2/3], Step [21800/25883], Loss: 2.1403, Perplexity: 8.50203\n",
      "Epoch [2/3], Step [21900/25883], Loss: 2.3709, Perplexity: 10.7067\n",
      "Epoch [2/3], Step [22000/25883], Loss: 2.5923, Perplexity: 13.3607\n",
      "Epoch [2/3], Step [22100/25883], Loss: 2.0723, Perplexity: 7.94317\n",
      "Epoch [2/3], Step [22200/25883], Loss: 2.3482, Perplexity: 10.4669\n",
      "Epoch [2/3], Step [22300/25883], Loss: 2.4467, Perplexity: 11.5499\n",
      "Epoch [2/3], Step [22400/25883], Loss: 2.6263, Perplexity: 13.8227\n",
      "Epoch [2/3], Step [22500/25883], Loss: 1.9970, Perplexity: 7.36707\n",
      "Epoch [2/3], Step [22600/25883], Loss: 2.0393, Perplexity: 7.68528\n",
      "Epoch [2/3], Step [22700/25883], Loss: 2.4169, Perplexity: 11.2111\n",
      "Epoch [2/3], Step [22800/25883], Loss: 2.4092, Perplexity: 11.1249\n",
      "Epoch [2/3], Step [22900/25883], Loss: 2.4439, Perplexity: 11.5180\n",
      "Epoch [2/3], Step [23000/25883], Loss: 2.3707, Perplexity: 10.70489\n",
      "Epoch [2/3], Step [23100/25883], Loss: 2.4478, Perplexity: 11.5623\n",
      "Epoch [2/3], Step [23200/25883], Loss: 1.9049, Perplexity: 6.71856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [23300/25883], Loss: 2.0628, Perplexity: 7.86809\n",
      "Epoch [2/3], Step [23400/25883], Loss: 2.4687, Perplexity: 11.8073\n",
      "Epoch [2/3], Step [23500/25883], Loss: 2.9717, Perplexity: 19.5259\n",
      "Epoch [2/3], Step [23600/25883], Loss: 2.6038, Perplexity: 13.5155\n",
      "Epoch [2/3], Step [23700/25883], Loss: 2.6017, Perplexity: 13.4864\n",
      "Epoch [2/3], Step [23800/25883], Loss: 2.1788, Perplexity: 8.83590\n",
      "Epoch [2/3], Step [23900/25883], Loss: 2.1041, Perplexity: 8.19946\n",
      "Epoch [2/3], Step [24000/25883], Loss: 2.2413, Perplexity: 9.40553\n",
      "Epoch [2/3], Step [24100/25883], Loss: 2.2811, Perplexity: 9.78791\n",
      "Epoch [2/3], Step [24200/25883], Loss: 2.9244, Perplexity: 18.6230\n",
      "Epoch [2/3], Step [24300/25883], Loss: 2.6248, Perplexity: 13.8012\n",
      "Epoch [2/3], Step [24400/25883], Loss: 2.1224, Perplexity: 8.350991\n",
      "Epoch [2/3], Step [24500/25883], Loss: 2.7125, Perplexity: 15.0676\n",
      "Epoch [2/3], Step [24600/25883], Loss: 2.3842, Perplexity: 10.8500\n",
      "Epoch [2/3], Step [24700/25883], Loss: 2.3223, Perplexity: 10.1993\n",
      "Epoch [2/3], Step [24800/25883], Loss: 2.2533, Perplexity: 9.51910\n",
      "Epoch [2/3], Step [24900/25883], Loss: 2.4633, Perplexity: 11.7435\n",
      "Epoch [2/3], Step [25000/25883], Loss: 1.9497, Perplexity: 7.02662\n",
      "Epoch [2/3], Step [25100/25883], Loss: 2.5100, Perplexity: 12.3055\n",
      "Epoch [2/3], Step [25200/25883], Loss: 2.7070, Perplexity: 14.9850\n",
      "Epoch [2/3], Step [25300/25883], Loss: 2.2477, Perplexity: 9.46587\n",
      "Epoch [2/3], Step [25400/25883], Loss: 2.4517, Perplexity: 11.6080\n",
      "Epoch [2/3], Step [25500/25883], Loss: 2.2532, Perplexity: 9.51819\n",
      "Epoch [2/3], Step [25600/25883], Loss: 2.9416, Perplexity: 18.9457\n",
      "Epoch [2/3], Step [25700/25883], Loss: 2.7354, Perplexity: 15.4160\n",
      "Epoch [2/3], Step [25800/25883], Loss: 2.5949, Perplexity: 13.3959\n",
      "Epoch [3/3], Step [100/25883], Loss: 1.9350, Perplexity: 6.9241973\n",
      "Epoch [3/3], Step [200/25883], Loss: 2.2576, Perplexity: 9.56015\n",
      "Epoch [3/3], Step [300/25883], Loss: 2.2304, Perplexity: 9.30367\n",
      "Epoch [3/3], Step [400/25883], Loss: 2.8856, Perplexity: 17.9140\n",
      "Epoch [3/3], Step [500/25883], Loss: 2.8884, Perplexity: 17.9650\n",
      "Epoch [3/3], Step [600/25883], Loss: 2.0611, Perplexity: 7.85474\n",
      "Epoch [3/3], Step [700/25883], Loss: 2.1863, Perplexity: 8.90237\n",
      "Epoch [3/3], Step [800/25883], Loss: 2.2601, Perplexity: 9.58404\n",
      "Epoch [3/3], Step [900/25883], Loss: 2.3148, Perplexity: 10.1225\n",
      "Epoch [3/3], Step [1000/25883], Loss: 2.7438, Perplexity: 15.5452\n",
      "Epoch [3/3], Step [1100/25883], Loss: 1.6701, Perplexity: 5.31270\n",
      "Epoch [3/3], Step [1200/25883], Loss: 2.1070, Perplexity: 8.22370\n",
      "Epoch [3/3], Step [1300/25883], Loss: 2.1964, Perplexity: 8.99273\n",
      "Epoch [3/3], Step [1400/25883], Loss: 2.3939, Perplexity: 10.9557\n",
      "Epoch [3/3], Step [1500/25883], Loss: 2.1451, Perplexity: 8.54298\n",
      "Epoch [3/3], Step [1600/25883], Loss: 2.0539, Perplexity: 7.79829\n",
      "Epoch [3/3], Step [1700/25883], Loss: 1.8980, Perplexity: 6.67286\n",
      "Epoch [3/3], Step [1800/25883], Loss: 2.4310, Perplexity: 11.3707\n",
      "Epoch [3/3], Step [1900/25883], Loss: 2.7614, Perplexity: 15.8214\n",
      "Epoch [3/3], Step [2000/25883], Loss: 2.4778, Perplexity: 11.9150\n",
      "Epoch [3/3], Step [2100/25883], Loss: 2.0610, Perplexity: 7.85367\n",
      "Epoch [3/3], Step [2200/25883], Loss: 2.0811, Perplexity: 8.01346\n",
      "Epoch [3/3], Step [2300/25883], Loss: 2.3100, Perplexity: 10.0746\n",
      "Epoch [3/3], Step [2400/25883], Loss: 2.3171, Perplexity: 10.1459\n",
      "Epoch [3/3], Step [2500/25883], Loss: 2.2460, Perplexity: 9.44978\n",
      "Epoch [3/3], Step [2600/25883], Loss: 2.0572, Perplexity: 7.82419\n",
      "Epoch [3/3], Step [2700/25883], Loss: 2.1203, Perplexity: 8.33321\n",
      "Epoch [3/3], Step [2800/25883], Loss: 2.4567, Perplexity: 11.6666\n",
      "Epoch [3/3], Step [2900/25883], Loss: 2.3803, Perplexity: 10.8080\n",
      "Epoch [3/3], Step [3000/25883], Loss: 1.8594, Perplexity: 6.420036\n",
      "Epoch [3/3], Step [3100/25883], Loss: 2.4664, Perplexity: 11.7798\n",
      "Epoch [3/3], Step [3200/25883], Loss: 2.3601, Perplexity: 10.5923\n",
      "Epoch [3/3], Step [3300/25883], Loss: 3.3694, Perplexity: 29.0625\n",
      "Epoch [3/3], Step [3400/25883], Loss: 2.4437, Perplexity: 11.5160\n",
      "Epoch [3/3], Step [3500/25883], Loss: 2.4071, Perplexity: 11.1017\n",
      "Epoch [3/3], Step [3600/25883], Loss: 2.7823, Perplexity: 16.1568\n",
      "Epoch [3/3], Step [3700/25883], Loss: 2.1649, Perplexity: 8.71379\n",
      "Epoch [3/3], Step [3800/25883], Loss: 2.0047, Perplexity: 7.42377\n",
      "Epoch [3/3], Step [3900/25883], Loss: 2.0901, Perplexity: 8.08598\n",
      "Epoch [3/3], Step [4000/25883], Loss: 2.1854, Perplexity: 8.89417\n",
      "Epoch [3/3], Step [4100/25883], Loss: 1.6759, Perplexity: 5.34341\n",
      "Epoch [3/3], Step [4200/25883], Loss: 2.2354, Perplexity: 9.35018\n",
      "Epoch [3/3], Step [4300/25883], Loss: 2.0308, Perplexity: 7.62040\n",
      "Epoch [3/3], Step [4400/25883], Loss: 2.2759, Perplexity: 9.73718\n",
      "Epoch [3/3], Step [4500/25883], Loss: 2.1702, Perplexity: 8.75978\n",
      "Epoch [3/3], Step [4600/25883], Loss: 2.2423, Perplexity: 9.41541\n",
      "Epoch [3/3], Step [4700/25883], Loss: 2.4671, Perplexity: 11.7886\n",
      "Epoch [3/3], Step [4800/25883], Loss: 2.2014, Perplexity: 9.03756\n",
      "Epoch [3/3], Step [4900/25883], Loss: 1.9261, Perplexity: 6.86273\n",
      "Epoch [3/3], Step [5000/25883], Loss: 1.8709, Perplexity: 6.49388\n",
      "Epoch [3/3], Step [5100/25883], Loss: 2.2442, Perplexity: 9.43289\n",
      "Epoch [3/3], Step [5200/25883], Loss: 2.5947, Perplexity: 13.3921\n",
      "Epoch [3/3], Step [5300/25883], Loss: 2.3692, Perplexity: 10.6885\n",
      "Epoch [3/3], Step [5400/25883], Loss: 2.3673, Perplexity: 10.6689\n",
      "Epoch [3/3], Step [5500/25883], Loss: 2.0557, Perplexity: 7.81197\n",
      "Epoch [3/3], Step [5600/25883], Loss: 2.5845, Perplexity: 13.2563\n",
      "Epoch [3/3], Step [5700/25883], Loss: 2.2272, Perplexity: 9.27364\n",
      "Epoch [3/3], Step [5800/25883], Loss: 2.4304, Perplexity: 11.3637\n",
      "Epoch [3/3], Step [5900/25883], Loss: 2.1876, Perplexity: 8.91393\n",
      "Epoch [3/3], Step [6000/25883], Loss: 2.3398, Perplexity: 10.3794\n",
      "Epoch [3/3], Step [6100/25883], Loss: 2.3867, Perplexity: 10.8771\n",
      "Epoch [3/3], Step [6200/25883], Loss: 2.2184, Perplexity: 9.19314\n",
      "Epoch [3/3], Step [6300/25883], Loss: 2.1057, Perplexity: 8.21307\n",
      "Epoch [3/3], Step [6400/25883], Loss: 2.3742, Perplexity: 10.7426\n",
      "Epoch [3/3], Step [6500/25883], Loss: 2.5877, Perplexity: 13.2995\n",
      "Epoch [3/3], Step [6600/25883], Loss: 2.7184, Perplexity: 15.1556\n",
      "Epoch [3/3], Step [6700/25883], Loss: 2.3212, Perplexity: 10.1881\n",
      "Epoch [3/3], Step [6800/25883], Loss: 2.6805, Perplexity: 14.5929\n",
      "Epoch [3/3], Step [6900/25883], Loss: 2.5587, Perplexity: 12.9195\n",
      "Epoch [3/3], Step [7000/25883], Loss: 2.1379, Perplexity: 8.48134\n",
      "Epoch [3/3], Step [7100/25883], Loss: 2.4204, Perplexity: 11.2499\n",
      "Epoch [3/3], Step [7200/25883], Loss: 2.3049, Perplexity: 10.0236\n",
      "Epoch [3/3], Step [7300/25883], Loss: 2.1439, Perplexity: 8.53291\n",
      "Epoch [3/3], Step [7400/25883], Loss: 2.2142, Perplexity: 9.15405\n",
      "Epoch [3/3], Step [7500/25883], Loss: 2.5916, Perplexity: 13.3508\n",
      "Epoch [3/3], Step [7600/25883], Loss: 2.7353, Perplexity: 15.4136\n",
      "Epoch [3/3], Step [7700/25883], Loss: 2.0258, Perplexity: 7.58217\n",
      "Epoch [3/3], Step [7800/25883], Loss: 2.5244, Perplexity: 12.4831\n",
      "Epoch [3/3], Step [7900/25883], Loss: 1.9974, Perplexity: 7.36970\n",
      "Epoch [3/3], Step [8000/25883], Loss: 2.2436, Perplexity: 9.42758\n",
      "Epoch [3/3], Step [8100/25883], Loss: 1.9498, Perplexity: 7.02731\n",
      "Epoch [3/3], Step [8200/25883], Loss: 2.1223, Perplexity: 8.35060\n",
      "Epoch [3/3], Step [8300/25883], Loss: 2.3248, Perplexity: 10.2246\n",
      "Epoch [3/3], Step [8400/25883], Loss: 2.6866, Perplexity: 14.6819\n",
      "Epoch [3/3], Step [8500/25883], Loss: 2.5819, Perplexity: 13.2222\n",
      "Epoch [3/3], Step [8600/25883], Loss: 2.3378, Perplexity: 10.3581\n",
      "Epoch [3/3], Step [8700/25883], Loss: 2.3890, Perplexity: 10.9027\n",
      "Epoch [3/3], Step [8800/25883], Loss: 2.3800, Perplexity: 10.8049\n",
      "Epoch [3/3], Step [8900/25883], Loss: 2.3103, Perplexity: 10.0776\n",
      "Epoch [3/3], Step [9000/25883], Loss: 1.9899, Perplexity: 7.31477\n",
      "Epoch [3/3], Step [9100/25883], Loss: 2.1905, Perplexity: 8.93964\n",
      "Epoch [3/3], Step [9200/25883], Loss: 1.9720, Perplexity: 7.18511\n",
      "Epoch [3/3], Step [9300/25883], Loss: 2.6299, Perplexity: 13.8718\n",
      "Epoch [3/3], Step [9400/25883], Loss: 2.1336, Perplexity: 8.44548\n",
      "Epoch [3/3], Step [9500/25883], Loss: 2.0604, Perplexity: 7.84927\n",
      "Epoch [3/3], Step [9600/25883], Loss: 2.8397, Perplexity: 17.1105\n",
      "Epoch [3/3], Step [9700/25883], Loss: 2.2668, Perplexity: 9.648875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [9800/25883], Loss: 1.9477, Perplexity: 7.01243\n",
      "Epoch [3/3], Step [9900/25883], Loss: 1.8409, Perplexity: 6.30221\n",
      "Epoch [3/3], Step [10000/25883], Loss: 2.4671, Perplexity: 11.7887\n",
      "Epoch [3/3], Step [10100/25883], Loss: 2.5906, Perplexity: 13.3375\n",
      "Epoch [3/3], Step [10200/25883], Loss: 2.0005, Perplexity: 7.39288\n",
      "Epoch [3/3], Step [10300/25883], Loss: 2.1788, Perplexity: 8.83603\n",
      "Epoch [3/3], Step [10400/25883], Loss: 2.3509, Perplexity: 10.4953\n",
      "Epoch [3/3], Step [10500/25883], Loss: 2.5131, Perplexity: 12.3431\n",
      "Epoch [3/3], Step [10600/25883], Loss: 2.1692, Perplexity: 8.75156\n",
      "Epoch [3/3], Step [10700/25883], Loss: 2.4086, Perplexity: 11.1181\n",
      "Epoch [3/3], Step [10800/25883], Loss: 2.4021, Perplexity: 11.0463\n",
      "Epoch [3/3], Step [10900/25883], Loss: 2.8693, Perplexity: 17.6244\n",
      "Epoch [3/3], Step [11000/25883], Loss: 1.8849, Perplexity: 6.58548\n",
      "Epoch [3/3], Step [11100/25883], Loss: 2.5123, Perplexity: 12.3328\n",
      "Epoch [3/3], Step [11200/25883], Loss: 2.4576, Perplexity: 11.6767\n",
      "Epoch [3/3], Step [11300/25883], Loss: 2.3832, Perplexity: 10.8399\n",
      "Epoch [3/3], Step [11400/25883], Loss: 2.2456, Perplexity: 9.44616\n",
      "Epoch [3/3], Step [11500/25883], Loss: 2.4454, Perplexity: 11.5352\n",
      "Epoch [3/3], Step [11600/25883], Loss: 2.3140, Perplexity: 10.1147\n",
      "Epoch [3/3], Step [11700/25883], Loss: 2.2008, Perplexity: 9.03197\n",
      "Epoch [3/3], Step [11800/25883], Loss: 1.8257, Perplexity: 6.20692\n",
      "Epoch [3/3], Step [11900/25883], Loss: 2.2937, Perplexity: 9.91196\n",
      "Epoch [3/3], Step [12000/25883], Loss: 2.1015, Perplexity: 8.17848\n",
      "Epoch [3/3], Step [12100/25883], Loss: 2.3153, Perplexity: 10.1281\n",
      "Epoch [3/3], Step [12200/25883], Loss: 2.3671, Perplexity: 10.6669\n",
      "Epoch [3/3], Step [12300/25883], Loss: 1.9686, Perplexity: 7.16040\n",
      "Epoch [3/3], Step [12400/25883], Loss: 1.9947, Perplexity: 7.35010\n",
      "Epoch [3/3], Step [12500/25883], Loss: 2.4128, Perplexity: 11.1650\n",
      "Epoch [3/3], Step [12600/25883], Loss: 2.2882, Perplexity: 9.85768\n",
      "Epoch [3/3], Step [12700/25883], Loss: 2.8168, Perplexity: 16.7232\n",
      "Epoch [3/3], Step [12800/25883], Loss: 2.2028, Perplexity: 9.05046\n",
      "Epoch [3/3], Step [12900/25883], Loss: 2.0292, Perplexity: 7.60818\n",
      "Epoch [3/3], Step [13000/25883], Loss: 2.2692, Perplexity: 9.67134\n",
      "Epoch [3/3], Step [13100/25883], Loss: 2.1988, Perplexity: 9.01399\n",
      "Epoch [3/3], Step [13200/25883], Loss: 2.0000, Perplexity: 7.38947\n",
      "Epoch [3/3], Step [13300/25883], Loss: 1.8409, Perplexity: 6.30222\n",
      "Epoch [3/3], Step [13400/25883], Loss: 2.2679, Perplexity: 9.65928\n",
      "Epoch [3/3], Step [13500/25883], Loss: 2.5099, Perplexity: 12.3040\n",
      "Epoch [3/3], Step [13600/25883], Loss: 2.3867, Perplexity: 10.8772\n",
      "Epoch [3/3], Step [13700/25883], Loss: 2.2129, Perplexity: 9.14204\n",
      "Epoch [3/3], Step [13800/25883], Loss: 2.0904, Perplexity: 8.088344\n",
      "Epoch [3/3], Step [13900/25883], Loss: 1.7598, Perplexity: 5.81131\n",
      "Epoch [3/3], Step [14000/25883], Loss: 2.1059, Perplexity: 8.21441\n",
      "Epoch [3/3], Step [14100/25883], Loss: 1.7554, Perplexity: 5.78562\n",
      "Epoch [3/3], Step [14200/25883], Loss: 2.2126, Perplexity: 9.13990\n",
      "Epoch [3/3], Step [14300/25883], Loss: 2.4987, Perplexity: 12.1663\n",
      "Epoch [3/3], Step [14400/25883], Loss: 1.9328, Perplexity: 6.90900\n",
      "Epoch [3/3], Step [14500/25883], Loss: 2.2571, Perplexity: 9.555191\n",
      "Epoch [3/3], Step [14600/25883], Loss: 2.2893, Perplexity: 9.86849\n",
      "Epoch [3/3], Step [14700/25883], Loss: 2.2543, Perplexity: 9.52871\n",
      "Epoch [3/3], Step [14800/25883], Loss: 2.0516, Perplexity: 7.78055\n",
      "Epoch [3/3], Step [14900/25883], Loss: 2.1715, Perplexity: 8.77179\n",
      "Epoch [3/3], Step [15000/25883], Loss: 2.0731, Perplexity: 7.94970\n",
      "Epoch [3/3], Step [15100/25883], Loss: 1.7775, Perplexity: 5.91485\n",
      "Epoch [3/3], Step [15200/25883], Loss: 1.8731, Perplexity: 6.50826\n",
      "Epoch [3/3], Step [15300/25883], Loss: 2.5138, Perplexity: 12.3515\n",
      "Epoch [3/3], Step [15400/25883], Loss: 2.3569, Perplexity: 10.5581\n",
      "Epoch [3/3], Step [15500/25883], Loss: 1.9049, Perplexity: 6.71899\n",
      "Epoch [3/3], Step [15600/25883], Loss: 1.9252, Perplexity: 6.85669\n",
      "Epoch [3/3], Step [15700/25883], Loss: 2.0843, Perplexity: 8.03910\n",
      "Epoch [3/3], Step [15800/25883], Loss: 2.3590, Perplexity: 10.5802\n",
      "Epoch [3/3], Step [15900/25883], Loss: 2.1401, Perplexity: 8.50073\n",
      "Epoch [3/3], Step [16000/25883], Loss: 1.8940, Perplexity: 6.64593\n",
      "Epoch [3/3], Step [16100/25883], Loss: 1.7746, Perplexity: 5.89770\n",
      "Epoch [3/3], Step [16200/25883], Loss: 2.2985, Perplexity: 9.95939\n",
      "Epoch [3/3], Step [16300/25883], Loss: 2.0853, Perplexity: 8.04687\n",
      "Epoch [3/3], Step [16400/25883], Loss: 2.1164, Perplexity: 8.30120\n",
      "Epoch [3/3], Step [16500/25883], Loss: 2.3524, Perplexity: 10.5105\n",
      "Epoch [3/3], Step [16600/25883], Loss: 1.9726, Perplexity: 7.18969\n",
      "Epoch [3/3], Step [16700/25883], Loss: 2.1197, Perplexity: 8.32821\n",
      "Epoch [3/3], Step [16800/25883], Loss: 2.0140, Perplexity: 7.49323\n",
      "Epoch [3/3], Step [16900/25883], Loss: 2.0141, Perplexity: 7.49375\n",
      "Epoch [3/3], Step [17000/25883], Loss: 1.7239, Perplexity: 5.60631\n",
      "Epoch [3/3], Step [17100/25883], Loss: 2.4296, Perplexity: 11.3548\n",
      "Epoch [3/3], Step [17200/25883], Loss: 2.4449, Perplexity: 11.5297\n",
      "Epoch [3/3], Step [17300/25883], Loss: 2.1505, Perplexity: 8.58944\n",
      "Epoch [3/3], Step [17400/25883], Loss: 1.9438, Perplexity: 6.98536\n",
      "Epoch [3/3], Step [17500/25883], Loss: 2.2274, Perplexity: 9.27553\n",
      "Epoch [3/3], Step [17600/25883], Loss: 2.3216, Perplexity: 10.1917\n",
      "Epoch [3/3], Step [17700/25883], Loss: 2.0620, Perplexity: 7.861727\n",
      "Epoch [3/3], Step [17800/25883], Loss: 2.1116, Perplexity: 8.26166\n",
      "Epoch [3/3], Step [17900/25883], Loss: 2.0326, Perplexity: 7.63419\n",
      "Epoch [3/3], Step [18000/25883], Loss: 2.1027, Perplexity: 8.18827\n",
      "Epoch [3/3], Step [18100/25883], Loss: 2.4555, Perplexity: 11.6526\n",
      "Epoch [3/3], Step [18200/25883], Loss: 2.1321, Perplexity: 8.43249\n",
      "Epoch [3/3], Step [18300/25883], Loss: 2.1491, Perplexity: 8.57714\n",
      "Epoch [3/3], Step [18400/25883], Loss: 1.9643, Perplexity: 7.12994\n",
      "Epoch [3/3], Step [18500/25883], Loss: 2.2807, Perplexity: 9.78335\n",
      "Epoch [3/3], Step [18600/25883], Loss: 2.2962, Perplexity: 9.93634\n",
      "Epoch [3/3], Step [18700/25883], Loss: 1.8501, Perplexity: 6.36072\n",
      "Epoch [3/3], Step [18800/25883], Loss: 2.2306, Perplexity: 9.30567\n",
      "Epoch [3/3], Step [18900/25883], Loss: 1.9947, Perplexity: 7.35027\n",
      "Epoch [3/3], Step [19000/25883], Loss: 2.1128, Perplexity: 8.27129\n",
      "Epoch [3/3], Step [19100/25883], Loss: 2.0839, Perplexity: 8.03563\n",
      "Epoch [3/3], Step [19200/25883], Loss: 2.0577, Perplexity: 7.82769\n",
      "Epoch [3/3], Step [19300/25883], Loss: 1.9792, Perplexity: 7.23715\n",
      "Epoch [3/3], Step [19400/25883], Loss: 1.8124, Perplexity: 6.12490\n",
      "Epoch [3/3], Step [19500/25883], Loss: 1.8034, Perplexity: 6.07016\n",
      "Epoch [3/3], Step [19600/25883], Loss: 2.6376, Perplexity: 13.9793\n",
      "Epoch [3/3], Step [19700/25883], Loss: 1.8086, Perplexity: 6.10164\n",
      "Epoch [3/3], Step [19800/25883], Loss: 2.1823, Perplexity: 8.86664\n",
      "Epoch [3/3], Step [19900/25883], Loss: 2.0815, Perplexity: 8.01649\n",
      "Epoch [3/3], Step [20000/25883], Loss: 3.6308, Perplexity: 37.7440\n",
      "Epoch [3/3], Step [20100/25883], Loss: 2.1999, Perplexity: 9.02440\n",
      "Epoch [3/3], Step [20200/25883], Loss: 3.2160, Perplexity: 24.9282\n",
      "Epoch [3/3], Step [20300/25883], Loss: 2.0991, Perplexity: 8.15860\n",
      "Epoch [3/3], Step [20400/25883], Loss: 2.1898, Perplexity: 8.93335\n",
      "Epoch [3/3], Step [20500/25883], Loss: 2.2395, Perplexity: 9.38878\n",
      "Epoch [3/3], Step [20600/25883], Loss: 2.2431, Perplexity: 9.42281\n",
      "Epoch [3/3], Step [20700/25883], Loss: 2.2378, Perplexity: 9.37302\n",
      "Epoch [3/3], Step [20800/25883], Loss: 2.1546, Perplexity: 8.62474\n",
      "Epoch [3/3], Step [20900/25883], Loss: 2.0524, Perplexity: 7.78659\n",
      "Epoch [3/3], Step [21000/25883], Loss: 2.3711, Perplexity: 10.7086\n",
      "Epoch [3/3], Step [21100/25883], Loss: 2.4475, Perplexity: 11.5593\n",
      "Epoch [3/3], Step [21200/25883], Loss: 2.6920, Perplexity: 14.7611\n",
      "Epoch [3/3], Step [21300/25883], Loss: 2.2134, Perplexity: 9.14713\n",
      "Epoch [3/3], Step [21400/25883], Loss: 2.0699, Perplexity: 7.92448\n",
      "Epoch [3/3], Step [21500/25883], Loss: 2.2386, Perplexity: 9.38023\n",
      "Epoch [3/3], Step [21600/25883], Loss: 2.1377, Perplexity: 8.47994\n",
      "Epoch [3/3], Step [21700/25883], Loss: 1.9764, Perplexity: 7.21681\n",
      "Epoch [3/3], Step [21800/25883], Loss: 3.9103, Perplexity: 49.9124\n",
      "Epoch [3/3], Step [21900/25883], Loss: 2.0276, Perplexity: 7.59586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [22000/25883], Loss: 2.5977, Perplexity: 13.4327\n",
      "Epoch [3/3], Step [22100/25883], Loss: 2.6270, Perplexity: 13.8324\n",
      "Epoch [3/3], Step [22200/25883], Loss: 2.0841, Perplexity: 8.03735\n",
      "Epoch [3/3], Step [22300/25883], Loss: 2.8242, Perplexity: 16.8478\n",
      "Epoch [3/3], Step [22400/25883], Loss: 2.2383, Perplexity: 9.37778\n",
      "Epoch [3/3], Step [22500/25883], Loss: 2.6696, Perplexity: 14.4345\n",
      "Epoch [3/3], Step [22600/25883], Loss: 2.1663, Perplexity: 8.72594\n",
      "Epoch [3/3], Step [22700/25883], Loss: 2.5344, Perplexity: 12.6092\n",
      "Epoch [3/3], Step [22800/25883], Loss: 2.3104, Perplexity: 10.0787\n",
      "Epoch [3/3], Step [22900/25883], Loss: 2.1554, Perplexity: 8.63165\n",
      "Epoch [3/3], Step [23000/25883], Loss: 2.0528, Perplexity: 7.78990\n",
      "Epoch [3/3], Step [23100/25883], Loss: 2.6730, Perplexity: 14.4831\n",
      "Epoch [3/3], Step [23200/25883], Loss: 2.3553, Perplexity: 10.5411\n",
      "Epoch [3/3], Step [23300/25883], Loss: 2.1088, Perplexity: 8.23845\n",
      "Epoch [3/3], Step [23400/25883], Loss: 2.0172, Perplexity: 7.51700\n",
      "Epoch [3/3], Step [23500/25883], Loss: 1.7798, Perplexity: 5.92866\n",
      "Epoch [3/3], Step [23600/25883], Loss: 2.2511, Perplexity: 9.49779\n",
      "Epoch [3/3], Step [23700/25883], Loss: 1.9296, Perplexity: 6.88660\n",
      "Epoch [3/3], Step [23800/25883], Loss: 2.4376, Perplexity: 11.4458\n",
      "Epoch [3/3], Step [23900/25883], Loss: 1.9632, Perplexity: 7.12227\n",
      "Epoch [3/3], Step [24000/25883], Loss: 2.0675, Perplexity: 7.90535\n",
      "Epoch [3/3], Step [24100/25883], Loss: 2.2732, Perplexity: 9.71056\n",
      "Epoch [3/3], Step [24200/25883], Loss: 2.4173, Perplexity: 11.2156\n",
      "Epoch [3/3], Step [24300/25883], Loss: 2.1203, Perplexity: 8.33338\n",
      "Epoch [3/3], Step [24400/25883], Loss: 2.7143, Perplexity: 15.0939\n",
      "Epoch [3/3], Step [24500/25883], Loss: 1.6590, Perplexity: 5.25420\n",
      "Epoch [3/3], Step [24600/25883], Loss: 1.8483, Perplexity: 6.34899\n",
      "Epoch [3/3], Step [24700/25883], Loss: 2.5804, Perplexity: 13.2021\n",
      "Epoch [3/3], Step [24800/25883], Loss: 2.5260, Perplexity: 12.5038\n",
      "Epoch [3/3], Step [24900/25883], Loss: 2.5480, Perplexity: 12.7819\n",
      "Epoch [3/3], Step [25000/25883], Loss: 1.8415, Perplexity: 6.30573\n",
      "Epoch [3/3], Step [25100/25883], Loss: 2.2159, Perplexity: 9.17011\n",
      "Epoch [3/3], Step [25200/25883], Loss: 2.4716, Perplexity: 11.8411\n",
      "Epoch [3/3], Step [25300/25883], Loss: 2.2815, Perplexity: 9.79140\n",
      "Epoch [3/3], Step [25400/25883], Loss: 2.1874, Perplexity: 8.91207\n",
      "Epoch [3/3], Step [25500/25883], Loss: 2.2625, Perplexity: 9.60663\n",
      "Epoch [3/3], Step [25600/25883], Loss: 2.1696, Perplexity: 8.75490\n",
      "Epoch [3/3], Step [25700/25883], Loss: 2.5284, Perplexity: 12.5333\n",
      "Epoch [3/3], Step [25800/25883], Loss: 2.1047, Perplexity: 8.20471\n",
      "Epoch [3/3], Step [25883/25883], Loss: 2.2855, Perplexity: 9.83035"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "# response = requests.request(\"GET\", \n",
    "#                             \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "#                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            # requests.request(\"POST\", \n",
    "            #                  \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "            #                  headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
